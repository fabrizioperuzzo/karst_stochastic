{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TODO\n",
    "\n",
    "- nel calcolo del diametro karst per le trincee considerare la distanza tra m_s-l_m e bedrock (h_quat+h_perm-h_rilevato)\n",
    "- introdurre una distribuzione exponenziale troncata\n",
    "- nel calcolo a ritroso dividere le subsidenze dagli inbuti\n",
    "- la def funziona per gli inbuti, fare una nuova def per le subsidenze\n",
    "- capire come gestire quegli inbuti che dal calcolo a ritroso non hanno soluzione (ex coperture alte)\n",
    "- cambiare nome h_rilevato con h_livelletta\n",
    "- in df0 dbase (contatto roccia) dave (superficie)\n",
    "- in df_zo dbase(superficie) dave(contatto roccia)\n",
    "- estrarre il num_karst medio in profondità (ad esempio per i viadotti e per stimare i riempimenti con il grout)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env conda run -n newenv3 python\n",
    "import sys\n",
    "print(sys.executable)    ### C:\\ProgramData\\Anaconda3_1\\envs\\newenv3\\python.exe\n",
    "#!{sys.executable} -m pip install tables              ##  df.to_hdf\n",
    "\n",
    "from platform import python_version\n",
    "print('The python_version is: ',python_version())\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "!jupyter kernelspec list\n",
    "# pip install xlrd\n",
    "# python -m pip install --user numpy scipy matplotlib ipython jupyter pandas sympy nose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "\n",
    "- Non può prevedere le caverne e gli eventi con probabilità bassa\n",
    "- L'assunzione principale si basa sul fatto che:\n",
    "        - i diametri di karst piccoli hanno una probabilità alta\n",
    "        - i diametri di karst grandi hanno una prob. bassa\n",
    "        - da qui l'assunzione di utilizzare una prob exp troncata ad un valore massimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from descartes import PolygonPatch\n",
    "import math\n",
    "from ipyleaflet import Map, GeoJSON\n",
    "import geojson\n",
    "import numpy as np\n",
    "#from mpl_toolkits.basemap import Basemap\n",
    "from ipyleaflet import Map\n",
    "import mpld3\n",
    "from dbfread import DBF\n",
    "mpld3.enable_notebook()\n",
    "#to import the basemap library give the direct path to the library\n",
    "# import os\n",
    "# os.environ[\"PROJ_LIB\"]=\"C:\\\\Users\\\\Anaconda3\\\\Library\\\\share\"\n",
    "import pandas as pd\n",
    "from simpledbf import Dbf5\n",
    "import math\n",
    "from Finddistribution import *\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import expon\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import types\n",
    "\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry.polygon import Polygon\n",
    "\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from scipy.interpolate import griddata\n",
    "from pyproj import Proj\n",
    "import plotly.express as px\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "from tqdm import trange\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scipy.stats import truncexpon\n",
    "\n",
    "import scipy.spatial as spatial\n",
    "from scipy.interpolate import griddata\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.patches import PathPatch\n",
    "\n",
    "from scipy.spatial import ConvexHull, convex_hull_plot_2d\n",
    "\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variabili globali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limite di altezza per distinguere tra viadotti alti e bridge bassi\n",
    "lim_via_bridge = 12.5\n",
    "\n",
    "vitautile = 100\n",
    "\n",
    "# soglia_dia_karst per differenziare karst small da karst grandi\n",
    "soglia_dia_karst = 3\n",
    "# 1000 numero di simulazioni stochastiche\n",
    "epochs = 100\n",
    "# metri in cui è diviso il file principale in punti \"Tabella_completa_karst2_3.csv\"\n",
    "m_punti = 20\n",
    "#109 serve per la routine principale\n",
    "pkfinale = 40\n",
    "# Groupby Accorpa in settori se uso: 50 ogni km, 5 ogni 100m\n",
    "div_settori = 50\n",
    "# correzione distr exp per aumentare stima diametri bassi\n",
    "# questo perché in teoria la distribuzione è exp troncata\n",
    "correzione_distr_exp = 2\n",
    "# soglia minima per fare la distribuzione esponenziale\n",
    "n_karst_soglia = 40\n",
    "# size per la distribuzione esponenziale\n",
    "size_exp = 1000\n",
    "##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHECK CONGRUENZA FILE INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop = pd.read_csv('df_prop.csv', sep=';', index_col=0)\n",
    "df_prop = df_prop.apply(pd.to_numeric, errors='ignore', downcast = 'float')\n",
    "df_prop.set_index('IGE_princi', inplace=True)\n",
    "\n",
    "if '3a' in df_prop.index: \n",
    "    print('OK: colonna index impostata correttamente come \"IGE_princi\"')\n",
    "else:\n",
    "    print('ERROR: colonna index potrebbe non avere gli IGE_princi')\n",
    "\n",
    "list_ige = ['3', '3a', '12', '6', '2a-1', '5-1', '2', '9', '6a', '6-1', '13', '11',\n",
    "       '4', '13-1', '17', '16', '19', '7', '18', '15a', '14', '340', '14-1',\n",
    "       '14-2', '14']\n",
    "err = 0\n",
    "for i in list_ige:\n",
    "    if i not in df_prop.index.tolist():\n",
    "        print(\"IGE \",i,\"ERROR: non è presente nella precedente lista di IGE\")\n",
    "        err = 1\n",
    "    else:\n",
    "        pass\n",
    "if err==0: print(\"OK: Tutti i nuovi 'IGE' erano presenti nella lista precedente\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# index nome IGE il resto del dataframe è tutto numerico\n",
    "df_prop_rock = pd.read_csv('df_prop_rock.csv', index_col=0)\n",
    "df_prop_rock = df_prop_rock.apply(pd.to_numeric, errors='coerce', downcast = 'float')\n",
    "\n",
    "if 'Gessi' in df_prop_rock.index: \n",
    "    print('OK: La colonna index del df_prop_rock è correttamente impostata sul tipo di roccia')\n",
    "else:\n",
    "    print('ERROR: La colonna index del df_prop_rock non è correttamente impostata sul tipo di roccia')\n",
    "\n",
    "\n",
    "df_orig = pd.read_csv('Tabella_completa_karst2_3.csv', sep=';', index_col=0)\n",
    "\n",
    "err = 0\n",
    "for i in df_orig.IGE.tolist():\n",
    "    if i not in df_prop.index.tolist():\n",
    "        print('ERROR: non corrisponde l IGE: ', i)\n",
    "        err += 1\n",
    "for i in df_orig.IGE_sec.tolist():\n",
    "    if i not in df_prop.index.tolist():\n",
    "        print('ERROR: non corrisponde l IGE: ', i)\n",
    "        err += 1\n",
    "if err == 0 : print('OK: Tutti gli IGE nel dataframe principale sono contenuti nel dataframe prop_list')\n",
    "    \n",
    "if df_prop.shape[0] == len(df_prop.index.unique().tolist()):\n",
    "    print('OK le IGE non sono ripetute sono univoche nel file df_prop')\n",
    "else:\n",
    "    print('ERROR gli IGE non sono univoci nel file df_prop')\n",
    "    \n",
    "    \n",
    "### df0 =======================================================================================\n",
    "\n",
    "df0 = pd.read_csv('kast-csv2.csv', sep=\";\")\n",
    "\n",
    "err = 0\n",
    "for i in ['long', 'lat', 'type', 'dave', 'era']:\n",
    "    if i in df0.columns.tolist():\n",
    "        pass\n",
    "    else:\n",
    "        print('ERROR The column -> ',i,' in df0 do not exists!')\n",
    "        err +=1\n",
    "if err==0: print('OK checked columns in df0')\n",
    "\n",
    "if df0.type.unique().tolist() == ['Inbuto', 'Subsidenza']: \n",
    "    print('OK checked column type of df0')\n",
    "else:\n",
    "    print('ERROR check column type of df0')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INIZIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = pd.read_csv('Tabella_completa_karst2_3.csv', sep=';', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# controlla quali colonne sono stringhe\n",
    "\n",
    "df_orig.select_dtypes('object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converti colonne che devono essere numeriche non necessario se la riga sopra fornisce un risultato corretto4\n",
    "\n",
    "col_list = ['Road_level','m_s_l_m_','calc_top','gessi_top','marne_top','Arg_perm','Arg_giura','h_perm','h_calc','h_gessi','h_giur','h_quat','h_marne']\n",
    "\n",
    "for col in col_list:\n",
    "    df_orig[col] = round(df_orig[col].astype('float'),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig['h_livelletta'] = df_orig['Road_level'] - df_orig['m_s_l_m_']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creo df_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_orig[['IGE', 'Litho', 'IGE_sec', 'IGE_SEC_PERC', 'LITHO_SEC', 'POINT_X', 'POINT_Y','h_quat','Tipo','Tipo_plus','h_rilevato']].copy()\n",
    "df = df_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # se non trova la colonna Tipo_plus usare questo script\n",
    "# df_temp_tipo = pd.read_csv('df_tipo_plus.csv', index_col=0)\n",
    "# df['Tipo_plus'] = df_temp_tipo['Tipo_plus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['h_perm'] = df_orig['h_perm'] + df_orig['h_giur'] + df_orig['h_marne']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(row):\n",
    "    if row['gessi_top'] > row['calc_top']:\n",
    "        val = 'g'\n",
    "    else:\n",
    "        val = 'c'\n",
    "    return val\n",
    "\n",
    "df['bedrock'] = df_orig.apply(f, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correggo gli spessori negativi per errore precisione Leader / profilo GEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['h_quat']<0,['h_quat']] = 0\n",
    "df.loc[df['h_perm']<0,['h_perm']] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carico file quote piano campagna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topography_lidar = pd.read_csv(\"Topography_Lidar_10m_vertices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topography_SRTM = pd.read_csv(\"SRTM_Karst_AREA vertices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creo Dataframe con le proprietà meccaniche  \"df_prop\" \"df_prop_rock\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop = pd.read_csv('df_prop.csv', sep=';', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Creo un dataframe con le proprietà meccaniche\n",
    "\n",
    "# index nome IGE il resto del dataframe è tutto numerico\n",
    "df_prop = pd.read_csv('df_prop.csv', sep=';', index_col=0)\n",
    "df_prop = df_prop.apply(pd.to_numeric, errors='ignore', downcast = 'float')\n",
    "df_prop.set_index('IGE_princi', inplace=True)\n",
    "\n",
    "\n",
    "# index nome IGE il resto del dataframe è tutto numerico\n",
    "df_prop_rock = pd.read_csv('df_prop_rock.csv', index_col=0)\n",
    "df_prop_rock = df_prop_rock.apply(pd.to_numeric, errors='coerce', downcast = 'float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_prop_base = [i if ((\"MAX\" not in i)&(\"MIN\" not in i)&(\"stdev\" not in i)) else \"\" for i in df_prop.columns.to_list()]\n",
    "col_prop_base = list(filter(None, col_prop_base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# litotipi associabili al permiano\n",
    "df_prop.loc[['14','16','17'],col_prop_base]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alcune colonne importanti continuano ad essere stringe per la presenza di cambi N/A di excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop.select_dtypes('object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prop_rock.select_dtypes('object').columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creo colonna progressive e gestisco punti tracciato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['POINT_X_Diff'] = df.POINT_X.diff().replace(np.nan, 0)\n",
    "df['POINT_Y_Diff'] = df.POINT_Y.diff().replace(np.nan, 0)\n",
    "df['length'] = np.sqrt(df.POINT_X_Diff**2 + df.POINT_Y_Diff**2).round().astype('int')\n",
    "df['progressiva'] = df.length.cumsum().round().astype('int')\n",
    "\n",
    "# correggo la prima lunghezza posso farlo dopo aver creato la colonna 'progressiva'\n",
    "df.loc[0, 'length'] = 20\n",
    "\n",
    "# verifico le colonne stringhe\n",
    "df.select_dtypes('object').columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calcololo offset del tracciato"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creo colonna con differenza ogni 2 punti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diff_X_roll2'] = df.POINT_X.diff(periods=2).shift(-1).fillna(method = 'bfill').fillna(method = 'ffill')\n",
    "df['diff_Y_roll2'] = df.POINT_Y.diff(periods=2).shift(-1).fillna(method = 'bfill').fillna(method = 'ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[[\n",
    " 'POINT_X',\n",
    " 'POINT_Y',\n",
    " 'diff_X_roll2',\n",
    " 'diff_Y_roll2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perpendicular(a) :\n",
    "    b = np.empty_like(a)\n",
    "    b[0] = a[1]\n",
    "    b[1] = -a[0]\n",
    "    return b\n",
    "\n",
    "def calcangle_2v(x):\n",
    "    v2 = np.array([0, 1])\n",
    "    v1 = x/np.linalg.norm(x)\n",
    "    dot_prod = np.dot(v1, v2)\n",
    "    return math.degrees(np.arccos(dot_prod))\n",
    "\n",
    "\n",
    "def offset(cx, cy, vx, vy, offset):\n",
    "\n",
    "    vdiff = np.array([vx, vy]).T\n",
    "    \n",
    "    # normalize the vector difference\n",
    "    # vdiffnorm = vdiff/(np.linalg.norm(vdiff))\n",
    "    # for row wise normalization need to set axis = 1 then to avoid errors on shape just use np.newaxis\n",
    "    vdiffnorm = vdiff/(np.linalg.norm(vdiff, axis=1))[:, np.newaxis]\n",
    "    # calculate its normalized perpendicular vector\n",
    "    \n",
    "    vnorm = np.apply_along_axis(perpendicular, 1, vdiffnorm)\n",
    "    angle = np.apply_along_axis(calcangle_2v, 1, vnorm)\n",
    "    voff = vnorm * offset\n",
    "    \n",
    "    vtrack = np.array([cx,cy]).T\n",
    "    vtrackup = vtrack + voff\n",
    "    vtrackdown = vtrack - voff\n",
    "    \n",
    "    #breakpoint()\n",
    "    \n",
    "    return vtrackup, vtrackdown, angle\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vtrackup, vtrackdown, angle = offset(df.POINT_X, df.POINT_Y, df1.diff_X_roll2, df1.diff_Y_roll2, 100 )\n",
    "vtrackup500, vtrackdown500, angle500 = offset(df.POINT_X, df.POINT_Y, df1.diff_X_roll2, df1.diff_Y_roll2, 500 )\n",
    "vtrackup2000, vtrackdown2000, angle2000 = offset(df.POINT_X, df.POINT_Y, df1.diff_X_roll2, df1.diff_Y_roll2, 2500 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creo colonne trackup , trackdown, 2000 500 100 nel df    (2000 è 2500m above below tracciato)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_ = ['vtrackupx','vtrackdownx','vtrackup500x','vtrackdown500x','vtrackup2000x','vtrackdown2000x']\n",
    "values_ = [vtrackup[:,0],vtrackdown[:,0],vtrackup500[:,0],vtrackdown500[:,0],vtrackup2000[:,0],vtrackdown2000[:,0]]\n",
    "for e,i in enumerate(columns_):\n",
    "    df[i] = values_[e]\n",
    "columns_ = ['vtrackupy','vtrackdowny','vtrackup500y','vtrackdown500y','vtrackup2000y','vtrackdown2000y']\n",
    "values_ = [vtrackup[:,1],vtrackdown[:,1],vtrackup500[:,1],vtrackdown500[:,1],vtrackup2000[:,1],vtrackdown2000[:,1]] \n",
    "for e,i in enumerate(columns_):\n",
    "    df[i] = values_[e]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creo le colonne utili per il calcolo delle contromisure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding for Tipe (rilevato, trincea, viadotto)\n",
    "df = pd.concat((df, pd.get_dummies(df['Tipo'])), axis=1)\n",
    "df = df.loc[:,~df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['h_rilevato_true'] = np.where((df.Tipo == 'Rilevato'), df['h_rilevato'],np.nan)\n",
    "df['h_trincea'] = np.where((df.Tipo == 'Trincea'), df['h_rilevato'],np.nan)\n",
    "df['h_viadotto'] = np.where(((df.Tipo != 'Trincea')&(df.Tipo != 'Rilevato')), df['h_rilevato'],np.nan)\n",
    "df['h_copertura'] = df.h_quat + df.h_perm\n",
    "df['h_copertura_ril'] = np.where((df.Tipo == 'Rilevato'), df['h_copertura'],np.nan)\n",
    "df['h_copertura_tri'] = np.where((df.Tipo == 'Trincea'), (df['h_copertura']+df['h_rilevato']),np.nan)\n",
    "df['h_copertura_via'] = np.where(((df.Tipo != 'Trincea')&(df.Tipo != 'Rilevato')), df['h_copertura'],np.nan)\n",
    "\n",
    "df['Viaduct'] = np.where(df['h_viadotto'] >= lim_via_bridge, df[\"Ponte/viadotto new\"], np.nan)\n",
    "df['Bridge'] = np.where(df['h_viadotto'] < lim_via_bridge, df[\"Ponte/viadotto new\"], np.nan)  \n",
    "df['LRilevato'] = df['Rilevato']*df['length']\n",
    "df['LTrincea'] = df['Trincea']*df['length']\n",
    "df['LViaduct'] = df['Viaduct']*df['length']\n",
    "df['LBridge'] = df['Bridge']*df['length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.progressiva>=21000)&(df.progressiva<22000)]['h_copertura_tri'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pulisco colonne inutili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['POINT_X_Diff','POINT_Y_Diff','diff_X_roll2','diff_Y_roll2'], inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carico il file con i Karst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df0 = pd.read_csv('karst.csv')\n",
    "df0 = pd.read_csv('kast-csv2.csv', sep=\";\")\n",
    "# df0.to_csv('kast-csv2.csv', sep=\";\")\n",
    "df0.select_dtypes('object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREO df_g50 --> df (ogni 20m) condensato ogni 1km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perché la media sia efficace bisogna convertire lo zero con np.nan dove ci sono delle colonne parziali\n",
    "# Attenzione quando condensi anche sul h_perm\n",
    "\n",
    "df['h_rilevato_true'] = df['h_rilevato_true'].replace(0, np.NaN)\n",
    "df['h_trincea'] = df['h_trincea'].replace(0, np.NaN)\n",
    "df['h_viadotto'] = df['h_viadotto'].replace(0, np.NaN)\n",
    "df['h_copertura_ril'] = df['h_copertura_ril'].replace(0, np.NaN)\n",
    "df['h_copertura_tri'] = df['h_copertura_tri'].replace(0, np.NaN)\n",
    "df['h_copertura_via'] = df['h_copertura_via'].replace(0, np.NaN)\n",
    "\n",
    "df_g50 = df.groupby(df.index // div_settori).agg({'progressiva':'first', 'POINT_X':'first', 'POINT_Y':'first', 'IGE':max, 'Litho':max, 'h_quat':'mean', 'h_perm':'mean',\n",
    "                                         'vtrackupx':'first', 'vtrackupy':'first', 'vtrackdownx':'first', 'vtrackdowny':'first',\n",
    "                                         'vtrackup500x':'first', 'vtrackup500y':'first', 'vtrackdown500x':'first', 'vtrackdown500y':'first',\n",
    "                                         'vtrackup2000x':'first', 'vtrackup2000y':'first', 'vtrackdown2000x':'first', 'vtrackdown2000y':'first',\n",
    "                                         'length':'sum','h_rilevato':'mean','h_rilevato_true':'mean','h_trincea':'mean','h_viadotto':'max',\n",
    "                                         'LRilevato':'sum','LTrincea':'sum','LViaduct':'sum','LBridge':'sum',\n",
    "                                         'h_copertura':'mean', 'h_copertura_ril':'mean', 'h_copertura_tri':'mean', 'h_copertura_via':'mean', 'Tipo_plus':'max'})\n",
    "# riaggiungo l'ultima riga\n",
    "\n",
    "df_g50 = df_g50.append(df[df_g50.columns.tolist()].tail(1), ignore_index=True, sort=False)\n",
    "\n",
    "# ripristino gli zeri sono obbligato perché il df lo uso per le interpolazioni\n",
    "df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g50.loc[[21]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_g50 = pd.read_csv('df_g50.csv', index_col=0, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONTO I KARST CHE CADONO NELL'AREA DI 1KM \n",
    "####  - aggiorno il df0 con i carst con la colonna \"sector2000_50\" che associa al karst un settore g50 di riferimento\n",
    "####  - aggiorno il df_g50 con le colonne 'n_karst' e 'sector_area'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g50['n_karst'] = np.nan\n",
    "df_g50['sector_area'] = np.nan\n",
    "df0['sector2000_50'] = np.nan\n",
    "df0['check_100'] = False\n",
    "df0['check_500'] = False\n",
    "\n",
    "for i in range(df_g50.shape[0]-1):\n",
    "    #######        prima escludevo tutti i punti oltre la pk 60000 adesso oltre la pk 110000\n",
    "    if (df_g50.progressiva[i] > 110000):\n",
    "        pass\n",
    "    \n",
    "    else:\n",
    "        polygon = Polygon([tuple(df_g50[['vtrackup2000x','vtrackup2000y']].loc[i,:]),\n",
    "                          tuple(df_g50[['vtrackup2000x','vtrackup2000y']].loc[i+1,:]),\n",
    "                          tuple(df_g50[['vtrackdown2000x','vtrackdown2000y']].loc[i+1,:]),\n",
    "                          tuple(df_g50[['vtrackdown2000x','vtrackdown2000y']].loc[i,:])])\n",
    "        polygon2 = Polygon([tuple(df_g50[['vtrackupx','vtrackupy']].loc[i,:]),\n",
    "                          tuple(df_g50[['vtrackupx','vtrackupy']].loc[i+1,:]),\n",
    "                          tuple(df_g50[['vtrackdownx','vtrackdowny']].loc[i+1,:]),\n",
    "                          tuple(df_g50[['vtrackdownx','vtrackdowny']].loc[i,:])])\n",
    "        polygon3 = Polygon([tuple(df_g50[['vtrackup500x','vtrackup500y']].loc[i,:]),\n",
    "                          tuple(df_g50[['vtrackup500x','vtrackup500y']].loc[i+1,:]),\n",
    "                          tuple(df_g50[['vtrackdown500x','vtrackdown500y']].loc[i+1,:]),\n",
    "                          tuple(df_g50[['vtrackdown500x','vtrackdown500y']].loc[i,:])])\n",
    "        n_karst = 0\n",
    "        \n",
    "        # ogni volta itero sul n° di karst di df0\n",
    "        # TODO : per ottimizzare se finisce in un settore rimuovo la riga dal dataframe\n",
    "        \n",
    "        for e in range(df0.shape[0]):\n",
    "            point = Point(df0.loc[df0.index[e],['long']], df0.loc[df0.index[e],['lat']])\n",
    "            if polygon.contains(point):\n",
    "                df0.loc[df0.index[e], 'sector2000_50'] = i\n",
    "                n_karst += 1\n",
    "            else:\n",
    "                pass\n",
    "            if polygon.contains(point): df0.loc[df0.index[e], 'check_2000'] = True\n",
    "            if polygon2.contains(point): df0.loc[df0.index[e], 'check_100'] = True\n",
    "            if polygon3.contains(point): df0.loc[df0.index[e], 'check_500'] = True\n",
    "        \n",
    "        df_g50.loc[df_g50.index[i], 'n_karst'] = n_karst\n",
    "        # area in km^2\n",
    "        df_g50.loc[df_g50.index[i], 'sector_area'] = polygon.area/1e6\n",
    "        df_g50.loc[df_g50.index[i], 'sector_area_100'] = polygon2.area/1e6\n",
    "        df_g50.loc[df_g50.index[i], 'sector_area_500'] = polygon3.area/1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g50['n_karst'] = df_g50['n_karst'].fillna(0)\n",
    "df_g50['sector_area'] = df_g50['sector_area'].fillna(method = 'ffill')   # oltre la pk50 tutto uguale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g50['n_karst_det'] = df_g50['n_karst']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop karst row outside 2500m boundary  (recall that sector2000 is 2500m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rimuovo i punti che finiscono fuori settore\n",
    "df0.dropna(subset=['sector2000_50'], inplace=True)\n",
    "df0['check_500'] = df0.check_500.fillna(False)\n",
    "df0['check_100'] = df0.check_100.fillna(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0['sector2000_50'] = df0.sector2000_50.astype('float')\n",
    "df0['sector2000_50_str'] = df0.sector2000_50.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creo colonna con gli indici dei karst  \"sector2000_50\" inizialmente è nell'indice ma la sposto in una colonna\n",
    "\n",
    "df_g50.reset_index(inplace = True)\n",
    "df_g50.rename(columns={'index':'sector2000_50'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g50.to_csv('df_g50.csv', sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREO LA FUNZIONE PER CALCOLARE DIA KARST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creo le funzioni di interpolazione per associare ad ogni karst di df0 delle proprieta: progressiva, h_rilevato, Tipo, IGE, h_perm ect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = df[['POINT_X','POINT_Y','h_perm','h_quat','IGE', 'IGE_sec', 'IGE_SEC_PERC','Tipo', 'Tipo_plus', 'h_rilevato_true', 'h_trincea', \n",
    "              'h_viadotto', 'h_copertura_tri', 'h_copertura_ril', 'h_copertura', 'h_copertura_via', 'h_rilevato', 'progressiva']].append(\n",
    "    df[['vtrackup2000x','vtrackup2000y','h_perm','h_quat','IGE', 'IGE_sec', 'IGE_SEC_PERC','Tipo', 'Tipo_plus', 'h_rilevato_true', 'h_trincea', \n",
    "        'h_viadotto', 'h_copertura_tri', 'h_copertura_ril', 'h_copertura', 'h_copertura_via', 'h_rilevato', 'progressiva']].rename(columns={'vtrackup2000x':'POINT_X','vtrackup2000y':'POINT_Y'}), \n",
    "    ignore_index=True).append(\n",
    "    df[['vtrackdown2000x','vtrackdown2000y','h_perm','h_quat','IGE', 'IGE_sec', 'IGE_SEC_PERC','Tipo', 'Tipo_plus', 'h_rilevato_true', 'h_trincea', \n",
    "        'h_viadotto', 'h_copertura_tri', 'h_copertura_ril', 'h_copertura', 'h_copertura_via', 'h_rilevato', 'progressiva']].rename(columns={'vtrackdown2000x':'POINT_X','vtrackdown2000y':'POINT_Y'}), \n",
    "    ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(points,values,points_pred):\n",
    "    '''\n",
    "    input\n",
    "     points = numpy.array(i,2)\n",
    "     values = numpy array(i,)\n",
    "     points_pred = numpy.array(i,2)\n",
    "    output\n",
    "     predicted_values \n",
    "    '''\n",
    "    points = np.array(points)\n",
    "    values = np.array(values)\n",
    "    points_pred = np.array(points_pred)\n",
    "    if (type(values[0].item()) == float) | (type(values[0].item()) == int):\n",
    "        values_pred = griddata(points, values, points_pred, method='linear')\n",
    "    else:\n",
    "        values_pred = griddata(points, values, points_pred, method='nearest')\n",
    "    \n",
    "    return values_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape, df_orig.shape, df_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = df_pred.loc[:,['POINT_X','POINT_Y']]\n",
    "values_h_perm = df_pred.loc[:,['h_perm']]\n",
    "values_h_quat = df_pred.loc[:,['h_quat']]\n",
    "values_ige = df_pred.loc[:,['IGE']]\n",
    "values_ige_sec = df_pred.loc[:,['IGE_sec']]\n",
    "values_ige_sec_perc = df_pred.loc[:,['IGE_SEC_PERC']]\n",
    "values_tipo = df_pred.loc[:,['Tipo']]\n",
    "values_tipo_plus = df_pred.loc[:,['Tipo_plus']]\n",
    "\n",
    "values_h_rilevato = df_pred.loc[:,['h_rilevato']]\n",
    "values_h_rilevato_true = df_pred.loc[:,['h_rilevato_true']]\n",
    "values_h_trincea = df_pred.loc[:,['h_trincea']]\n",
    "values_h_viadotto = df_pred.loc[:,['h_viadotto']]\n",
    "\n",
    "values_h_copertura = df_pred.loc[:,['h_copertura']]\n",
    "values_h_copertura_ril = df_pred.loc[:,['h_copertura_ril']]\n",
    "values_h_copertura_tri = df_pred.loc[:,['h_copertura_tri']]\n",
    "values_h_copertura_via = df_pred.loc[:,['h_copertura_via']]\n",
    "\n",
    "values_progressiva = df_pred.loc[:,['progressiva']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_pred = df0.loc[:,['long','lat']]\n",
    "df0['h_perm'] = predict(points, values_h_perm, point_pred)\n",
    "df0['h_quat'] = predict(points, values_h_quat, point_pred)\n",
    "df0['IGE'] = predict(points, values_ige, point_pred)\n",
    "df0['IGE_sec'] = predict(points, values_ige_sec, point_pred)\n",
    "df0['IGE_SEC_PERC'] = predict(points, values_ige_sec_perc, point_pred)\n",
    "df0['progressiva'] = predict(points, values_progressiva, point_pred)\n",
    "df0['Litho'] = df_prop.lithotipo[df0.IGE].tolist()\n",
    "\n",
    "df0['Tipo'] = predict(points, values_tipo, point_pred)\n",
    "df0['Tipo_plus'] = predict(points, values_tipo_plus, point_pred)\n",
    "df0['h_copertura'] = predict(points, values_h_copertura, point_pred)\n",
    "df0['h_copertura_ril'] = predict(points, values_h_copertura_ril, point_pred)\n",
    "df0['h_copertura_tri'] = predict(points, values_h_copertura_tri, point_pred)\n",
    "df0['h_copertura_via'] = predict(points, values_h_copertura_via, point_pred)\n",
    "df0['h_rilevato'] = predict(points, values_h_rilevato, point_pred)\n",
    "df0['h_rilevato_true'] = predict(points, values_h_rilevato_true, point_pred)\n",
    "df0['h_trincea'] = predict(points, values_h_trincea, point_pred)\n",
    "df0['h_viadotto'] = predict(points, values_h_viadotto, point_pred)\n",
    "\n",
    "df0['vol_dave'] = 4/3*np.pi*(df0.dave/2)**3 / 2\n",
    "df0.fillna(0, inplace=True)\n",
    "# attribuisco ad ogni karst il lithotipo principale\n",
    "d1 = dict(zip(df_prop.index, df_prop.lithotipo))\n",
    "df0['Litho'] = df0.replace({'IGE':d1})['IGE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with plt.style.context((\"seaborn-pastel\",)):\n",
    "    fig, ax = plt.subplots(1,1, figsize=(22,4))\n",
    "    ax.plot(df.POINT_X, df.progressiva)\n",
    "    ax.scatter(df0.long, df0.progressiva, c='navy')\n",
    "#     plt.xlim(df.POINT_X.min(), df[df.progressiva<40000]['POINT_X'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with plt.style.context((\"seaborn-pastel\",)):\n",
    "    fig, ax = plt.subplots(1,1, figsize=(22,4))\n",
    "    ax.plot(df.POINT_X, df.IGE)\n",
    "    ax.scatter(df0.long, df0.IGE, c='navy')\n",
    "    plt.xlim(df.POINT_X.min(), df[df.progressiva<40000]['POINT_X'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  ATTENZIONE PER AVERE UNA BUONA CORRISPONDENZA KARST VS LITOGRAFIA PLOTTARE I KARST CHE SONO PROSSIMI AL TRACCIATO!!! \n",
    "#check 100 = True troppi pochi punti\n",
    "# con check 500 si ha un buon compromesso tra distorsione data dalla distanza dall'asse e il numero di karst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with plt.style.context((\"dark_background\",)):\n",
    "with plt.style.context((\"seaborn-pastel\",)):\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1, figsize=(24,4))\n",
    "    ax.plot(df.POINT_X, df.h_quat, label='quat thickness')\n",
    "    ax.plot(df.POINT_X, df.h_perm, c='m', label='permian thickness')\n",
    "    \n",
    "    # se uso check  100 troppi pochi karst\n",
    "    #ax.scatter(df0[df0.check_100 == True].long, df0[df0.check_100 == True].h_quat)\n",
    "    #ax.scatter(df0.long, df0.h_quat)\n",
    "    ax.scatter(df0[df0.check_500 == True].long, df0[df0.check_500 == True].h_quat)\n",
    "    \n",
    "    plt.legend()\n",
    "\n",
    "    \n",
    "    plt.xlim(df.POINT_X.min(), df[df.progressiva<40000]['POINT_X'].max())\n",
    "    \n",
    "    plt.savefig('Karst_andprofile.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context((\"seaborn-pastel\",)):\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1, figsize=(24,4))\n",
    "    ax.plot(df.POINT_X, df.h_quat+df.h_perm, label='quat+perm thickness')\n",
    "    ax.hlines(50,df.POINT_X.min(), df.POINT_X.max(), colors=\"m\")\n",
    "    \n",
    "    #ax.scatter(df0.long, np.ones(df0.shape[0])*50, c=\"m\", label=\"Karsts and subsidences\")\n",
    "    ax.scatter(df0[df0.check_500 == True].long, np.ones(df0[df0.check_500 == True].shape[0]), c=\"m\", label=\"Karsts and subsidences\")\n",
    "    \n",
    "    plt.legend()\n",
    "\n",
    "    \n",
    "    plt.xlim(df.POINT_X.min(), df[df.progressiva<40000]['POINT_X'].max())\n",
    "    \n",
    "    plt.savefig('Karst_andprofile_50mline.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with plt.style.context((\"dark_background\",)):\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1, figsize=(24,4))\n",
    "    ax.plot(df.POINT_X, df_orig.m_s_l_m_, c=\"w\", label=\"Ground Level\")\n",
    "    ax.plot(df.POINT_X, df_orig.calc_top, c=\"b\", label=\"BedRock\")\n",
    "    ax.plot(df.POINT_X, df_orig.Arg_perm, c=\"c\", label=\"Permian\")\n",
    "\n",
    "    #ax.scatter(df0.long, np.ones(df0.shape[0])*50, c=\"m\", label=\"Karsts and subsidences\")\n",
    "    ax.scatter(df0[df0.check_500 == True].long, np.ones(df0[df0.check_500 == True].shape[0])*50, c=\"m\", label=\"Karsts and subsidences\")\n",
    "    \n",
    "    \n",
    "    plt.legend()\n",
    "\n",
    "    \n",
    "    plt.xlim(df.POINT_X.min(), df[df.progressiva<40000]['POINT_X'].max())\n",
    "    plt.savefig('Karst_andprofile_legend.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    " \n",
    "# plotting both distibutions on the same figure\n",
    "fig = sns.barplot(df0[df0.Litho=='s'].type, df0[df0.Litho=='s'].dave, color=\"r\")\n",
    "# fig = sns.barplot(df0[df0.Litho=='a'].type, df0[df0.Litho=='a'].dave, color=\"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcolo Karst alla base con formula inversa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FUNZIONE PER IL CALCOLO INVERSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dia(row, t, df_prop, df_prop_rock):\n",
    "    \n",
    "    # il diametro iniziale \"d0\" = 0.1 incrementato per step\n",
    "    # when doing back calculation use t= 0\n",
    "    \n",
    "    lista_argille = df_prop[df_prop.lithotipo == 'a'].index.tolist()\n",
    "    lista_sabbie = df_prop[df_prop.lithotipo == 's'].index.tolist()\n",
    "    \n",
    "    #==========================================================================================\n",
    "    #                   QUI CREO LA LISTA CON GLI IGE IN QUEL KARST SPECIFICO\n",
    "    #           PUO' ESSERE MESSA COME DATO DI INPUT SE INTERPOLATA ESTERNAMENTE\n",
    "    #   OPPURE PUO ESSERE CREATA QUI DI SEGUITO PER INTERPOLAZIONE CON I BOREHOLES FITTIZI\n",
    "    \n",
    "    if row.h_perm>0:\n",
    "        # TODO chiedere a Walter il permiano a che unità corrisponde\n",
    "        # 14 --> J3 -->  \"Clay grayish-brown, dusty, light, hard, J3\n",
    "        list_layer = ['17']\n",
    "        list_lathick = [row.h_perm]\n",
    "    else:\n",
    "        list_layer = []\n",
    "        list_lathick  =[]\n",
    "\n",
    "    if row.h_quat < 5:\n",
    "        list_layer.append(row.IGE)\n",
    "        list_lathick.append(row.h_quat)\n",
    "    else:\n",
    "        # se la litologia principale è argilla\n",
    "        if df_prop.loc[row.IGE, 'lithotipo'] == 'a':\n",
    "            # DO IT\n",
    "            # crea una funzione che interpone un litotipo in sabbia\n",
    "            # di spessore random uniform da 1/10 a 1/5 dello spessore totale dell'argilla\n",
    "            list_layer.append(row.IGE)\n",
    "            #list_layer.append(random.choice(lista_sabbie))\n",
    "            list_layer.append(row.IGE_sec)\n",
    "            list_layer.append(row.IGE)\n",
    "            ige_perc = row.IGE_SEC_PERC\n",
    "            randthick = random.uniform(row.h_quat*ige_perc*1.2, row.h_quat*ige_perc*0.8)\n",
    "            list_lathick.append((row.h_quat - randthick)/2)\n",
    "            list_lathick.append(randthick)\n",
    "            list_lathick.append((row.h_quat - randthick)/2)\n",
    "        else:  \n",
    "            # se la litologia principale è sabbia\n",
    "            # DO IT\n",
    "            # crea una funzione che interpone un litotipo in argilla\n",
    "            # di spessore random uniform da 1/10 a 1/5 dello spessore totale della sabbia  \n",
    "            list_layer.append(row.IGE)\n",
    "            list_layer.append(random.choice(lista_argille))\n",
    "            list_layer.append(row.IGE)\n",
    "            randthick = random.uniform(row.h_quat/10, row.h_quat/5)\n",
    "            list_lathick.append((row.h_quat - randthick)/2)\n",
    "            list_lathick.append(randthick)\n",
    "            list_lathick.append((row.h_quat - randthick)/2)\n",
    "    \n",
    "    list_sv = []\n",
    "    sv_temp = 0\n",
    "    # invertire layer e calc sv\n",
    "    for i in range(len(list_layer)):\n",
    "        sv_temp += df_prop.gamma[i] * list_lathick[-1*(1+i)]\n",
    "        list_sv.append(sv_temp)\n",
    "    \n",
    "    # come per list_layer ordino dal basso verso l'alto\n",
    "    list_sv.reverse()\n",
    "  \n",
    "    #print(list_layer, list_lathick, df_prop.lithotipo[list_layer].tolist())\n",
    "    \n",
    "    # a questo punto ho la lista con litotipi e spessori\n",
    "    # per le rocce usa solo calcari ma tieni in conto una futura revisione\n",
    "    \n",
    "    d_0 = 0.33\n",
    "    d_target = row.dave\n",
    "    sv = 0   # sigma inizializzata a zero\n",
    "    \n",
    "    # per la versione a ritroso d0 =0.1 e incrementato di 0.1 fin quando al top dello strato non si trova\n",
    "    # lo stesso valore di dave\n",
    "\n",
    "    \n",
    "    #=================================================   inizio calcdia_strato   =================================================\n",
    "    \n",
    "    def calcdia_strato(df_prop, d_pod, layer_i, thick_i, sv, sv_i):\n",
    "        # creare una funzione che restituisce il diametro strato superiore        \n",
    "        # return i parametri che si aggiornano: d_top e sv \n",
    "        \n",
    "        primoloop = True if sv == 0 else False\n",
    "        \n",
    "        radphi = np.radians(df_prop.phi[layer_i])\n",
    "        _gamma = df_prop.gamma[layer_i]\n",
    "        \n",
    "        z1 = d_pod/2 * (1+2*np.sin(radphi))/(2*np.tan(radphi)*np.sin(radphi))\n",
    "        \n",
    "        x_2 = d_pod/2 * (1 + np.sin(radphi))**2/(4*np.sin(radphi)**3)\n",
    "        X_2 = x_2 * 2\n",
    "        \n",
    "        z2 = z1 + d_pod/2*(1+np.sin(radphi))/(4*np.tan(radphi)*np.sin(radphi)**2)\n",
    "\n",
    "        z3 = z2 + x_2/(np.tan(radphi))\n",
    "\n",
    "        \n",
    "        \n",
    "        zone = 1 if(thick_i < z2) else (2 if thick_i < z3 else 3)   \n",
    "        \n",
    "        \n",
    "        #sv = sv + df_prop.gamma[layer_i] * thick_i\n",
    "        sv = 1 # per indicare loop successivi al primo\n",
    "            \n",
    "        eta = 0\n",
    "       \n",
    "        if df_prop.lithotipo[layer_i] == 'a':\n",
    "            \n",
    "            '''\n",
    "            ********* ARGILLE  *******\n",
    "            '''\n",
    "                  \n",
    "            _IL = float(df_prop.IL[layer_i])\n",
    "            _c = df_prop.c[layer_i]\n",
    "            if _c == 0: _c = 1\n",
    "            _dfis = df_prop.Dfis[layer_i]\n",
    "            \n",
    "\n",
    "            dcrit = 2 * thick_i * ((np.tan(np.pi/4-radphi/2))**2 * np.tan(radphi) + 2 *_c * (1 - _IL)/sv_i)\n",
    "            if dcrit < 0.01: dcrit = 0.01\n",
    "            dpipe = np.exp(_IL) * _dfis * _gamma * thick_i / _c\n",
    "            \n",
    "            # parametro per il calcolo Pds\n",
    "            eta = thick_i * dpipe / dcrit\n",
    "            \n",
    "            ## primo loop\n",
    "            \n",
    "            d_top = d_pod\n",
    "            \n",
    "            if (not primoloop) and (d_pod > dpipe): \n",
    "                if d_pod > dcrit:\n",
    "                    d_top = dcrit\n",
    "                else:\n",
    "                    d_top = dpipe\n",
    "            else:\n",
    "                #d_top = d_pod   ********** ATTENZIONE ************\n",
    "                d_top = d_pod\n",
    "                pass\n",
    "        \n",
    "        else:\n",
    "            '''\n",
    "            *********  SABBIE   *******\n",
    "            '''\n",
    "            \n",
    "            if zone == 1 :\n",
    "                d_top = (X_2 - d_pod) / z2 * thick_i + d_pod\n",
    "                #print('zone1 :',zone,'z2',z2,'z3',z3,'dpod: ',d_pod, 'phi: ', df_prop.phi[layer_i], 'radphi: ', radphi, 'gamma: ', _gamma)\n",
    "            elif zone == 2 :\n",
    "                d_top = X_2\n",
    "                #print('zone2 :',zone,'z2',z2,'z3',z3,'dpod: ',d_pod, 'phi: ', df_prop.phi[layer_i], 'radphi: ', radphi, 'gamma: ', _gamma)\n",
    "            else :\n",
    "                #d_top = 0    ********** ATTENZIONE ************\n",
    "                d_top = X_2\n",
    "                #print('zone3 :',zone,'z2',z2,'z3',z3,'dpod: ',d_pod, 'phi: ', df_prop.phi[layer_i], 'radphi: ', radphi, 'gamma: ', _gamma)\n",
    "        \n",
    "        vol = np.pi * thick_i / 3 * (d_pod**2 / 4 + d_pod * d_top / 4 + d_top**2 / 4)\n",
    "        \n",
    "        if zone == 3: vol = vol + np.pi * d_pod**2 * (thick_i - z3) / 4\n",
    "            \n",
    "        e_i = float(df_prop.e[layer_i])\n",
    "        ecr_i = float(df_prop[\"e,cr\"][layer_i])\n",
    "            \n",
    "        deltav = (ecr_i - e_i) /(1 + ecr_i) * vol\n",
    "        \n",
    "        return d_top, sv, vol, eta\n",
    "    \n",
    "    #===========================================  end calcdia_strato      ============================================================  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #================================================================================================================================        \n",
    "        \n",
    "    def calc_fixed_par(t, df_prop_rock, d_0, list_layer):\n",
    "        \n",
    "        # when doing back calculation use t=0\n",
    "        \n",
    "        v_r = df_prop_rock.v_r.Calcari\n",
    "        h_p = d_0/2.5  # da controllare\n",
    "        D1 = v_r * t + d_0\n",
    "        m_k = df_prop_rock.m_k.Calcari\n",
    "        k_k = df_prop_rock.k_k.Calcari\n",
    "        phiradmedio = df_prop.phi[list_layer].mean()\n",
    "\n",
    "        return(v_r, h_p, D1, m_k, k_k, phiradmedio)\n",
    "    \n",
    "    #===========================================   end  calc_fixed_par       =======================================================  \n",
    "    \n",
    "    \n",
    "    v_r, h_p, d_0, m_k, k_k, phiradmedio  = calc_fixed_par(t, df_prop_rock, d_0, list_layer)  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #===========================================   inizio ciclo iterativo       =======================================================\n",
    "    step = 1\n",
    "    report = 'other'\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        # inizializzo a d_0 finito il ciclo sarà quello in superficie\n",
    "        d_top = d_0\n",
    "           \n",
    "        list_dia = [d_0] # il primo della lista è il d_pod\n",
    "        list_vol = []\n",
    "        list_eta = []\n",
    "        \n",
    "        for i, e in enumerate(list_layer):\n",
    "            \n",
    "            # calcola per tutti i layer quando esce dal loop d_0 = diam al piano campagna\n",
    "            layer_i = e\n",
    "            thick_i = list_lathick[i]\n",
    "            sv_i = list_sv[i]\n",
    "            \n",
    "            d_top, sv, vol, eta = calcdia_strato(df_prop, d_top, layer_i, thick_i, sv, sv_i)\n",
    "            \n",
    "            list_dia.append(d_top)\n",
    "            list_vol.append(vol)\n",
    "            list_eta.append(eta)\n",
    "            \n",
    "        # calc subsidence\n",
    "        \n",
    "        dpod1 = list_dia[-2] # devo prendere la penultima perché l'ultima è in superficie\n",
    "        sv1 = list_sv[-1]    # sv' al primo strato in alto (ultimo nella lista)\n",
    "        lay1 = list_layer[-1] # layer del primo strato ovvero l'IGE principale\n",
    "        th1  = list_lathick[-1] # thickness del primo strato superficiale (ultimo nella lista)\n",
    "        \n",
    "        d_sub = 2 * np.sqrt(th1**2 - (dpod1/2)**2)\n",
    "        \n",
    "        # per spessori piccoli th1 < raggio-pod quindi non c'è soluzione imporre un diametro minimo d_0 = 0.33\n",
    "        # aumentando il dpod ad un certo punto ottengo Dpod/2 > th1 --> Nan\n",
    "        # impondo il break\n",
    "        # per entrare nel break impongo\n",
    "        \n",
    "        if (th1**2 - (dpod1/2)**2)<0: d_sub = d_target\n",
    "        \n",
    "            \n",
    "        \n",
    "        h_sub = sv1 * dpod1**3 * th1 * (1-df_prop.nu[lay1]**2) * (1+((df_prop.nu[lay1])/(1-df_prop.nu[lay1]))\n",
    "                                                                 ) / (df_prop.E[lay1] * 1000 * ((4 * dpod1**2) - th1**2))\n",
    "        # per evitare soluzioni negative\n",
    "        if ((4 * dpod1**2) - th1**2) <= 0: h_sub = 100\n",
    "               \n",
    "        # d_0 è il d alla base stimato\n",
    "        # d_top è quello calcolato a piano campagna (che appena supera il d_target stoppa i calcoli)\n",
    "        \n",
    "        #if (d_top < d_target) & (d_0 < d_target) & (row.type == 'Inbuto'):\n",
    "        if (d_top < d_target) & (d_0 < d_target):\n",
    "            step += 1\n",
    "            d_0 += 0.33\n",
    "            \n",
    "            # ricomincia il loop da while con il valore d_0 incrementato\n",
    "            if step >= 100: \n",
    "                d_0 = d_target\n",
    "                #print('Risultato finale max iteration: ', d_0, d_top, list_dia)\n",
    "                report = ('100_iter')\n",
    "                break\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        #elif (d_sub < d_target) & (d_0 < d_target) & (row.type != 'Inbuto'):\n",
    "        elif (d_sub < d_target) & (d_0 < d_target):\n",
    "            step += 1\n",
    "            d_0 += 0.33\n",
    "            \n",
    "            # ricomincia il loop da while con il valore d_0 incrementato\n",
    "            if step >= 100: \n",
    "                d_0 = d_target\n",
    "                #print('Risultato finale max iteration: ', d_0, d_top, list_dia)\n",
    "                report = ('100_iter')\n",
    "                break\n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "        else:\n",
    "            # esci dal loop il primo valore d_0 che porta d_top > d_target è quello finale\n",
    "            #print('Risultato finale : ', d_0, d_top, list_dia)\n",
    "            report = ('sltn_found')\n",
    "            break\n",
    "    \n",
    "    \n",
    "    #===========================================   fine ciclo iterativo       =======================================================\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #===========================================    inizio calcprob         =========================================================    \n",
    "    \n",
    "    def calc_prob(df_prop, df_prop_rock, list_layer, d_top, v_r, h_p, D1, m_k, k_k, phiradmedio, t, list_vol, list_eta):\n",
    "\n",
    "        # H_k = non richiesta\n",
    "        \n",
    "        Va_0 = np.pi * D1**2 * h_p/4\n",
    "        \n",
    "        Va_1 = np.pi * m_k * (k_k + v_r * t / m_k) * (3*D1**2 + ( 2 * m_k * (3*D1 + \\\n",
    "                2 * m_k / np.tan(phiradmedio)) / (np.tan(phiradmedio))))/12\n",
    "\n",
    "        Va = Va_0 + Va_1\n",
    "        \n",
    "        list_Va0i = list_vol + Va_0\n",
    "        \n",
    "        sum_Va0i = np.array(list_Va0i).sum()\n",
    "        sum_eta = np.array(list_eta).sum()\n",
    "        sum_m = np.array(list_lathick).sum()\n",
    "\n",
    "        Pds = 1 - np.exp(-Va / sum_Va0i * sum_eta / sum_m)\n",
    "\n",
    "        return Pds\n",
    "    \n",
    "    #===========================================     fine  calcprob         ========================================================= \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # Nel calcolo non a ritroso se Pds < 0.3 --> d_0 = 0\n",
    "    \n",
    "    #Pds = calc_prob(df_prop, df_prop_rock, list_layer, d_target, v_r, h_p, d_0, m_k, k_k, phiradmedio, t, list_vol, list_eta)\n",
    "    \n",
    "    if (row.h_perm == 0) & (row.h_quat <1) : d_0 = d_target\n",
    "    \n",
    "    #print(\"\\n\", \"===========================================================================================================\", \"\\n\")\n",
    "    \n",
    "    return pd.Series([d_0, d_sub, h_sub, step, report])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nel df0 dbase è diametro al contatto con la roccia mentre nel df_zo è il diametro in superficie!\n",
    "\n",
    "df0[['dbase', 'd_sub', 'h_sub', 'step', 'report']] = df0.apply(calculate_dia, axis=1, args=(0, df_prop, df_prop_rock))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0['vol_dbase'] = 4/3*np.pi*(df0.dbase/2)**3 / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DA QUI INIZIANO LE 1000 SIMULAZIONI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- n_karst_soglia confermato = 70 (totale giovani + antichi)\n",
    "- ritorno a considerare solo quelli giovani\n",
    "- confermo : permiano > 40m d_top = 0\n",
    "- confermo : permiano + quat > 50m d_top = 0\n",
    "- la funzione calcola strato return d_top e Pds\n",
    "- creo prima il df_zo poi seleziono il check100 per creare df_zo_100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TROVA DISTRIBUZIONE PER OGNI ZONA E CREA STOCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trova_distr_exp(df_g50, hh = 'none'):\n",
    "\n",
    "    # cerco l'area con più karst\n",
    "    # e quella con karst minimo (il minimo viene definito qui)\n",
    "    # sotto un certo numero di karst per area uso una quantità minima prefissata\n",
    "    n_karst_soglia = n_karst_soglia\n",
    "    #size dei dati simulati\n",
    "    size = size_exp\n",
    "    # numero max in tutti i settori di 1km\n",
    "    n_max_settore = int(df_g50.n_karst.max())\n",
    "    # settore con numero max\n",
    "    settore_nmax= df_g50[df_g50.n_karst == df_g50.n_karst.max()].sector2000_50.tolist()[0]\n",
    "    # settore con numero karst basso ma maggiore della soglia (ex 40):\n",
    "    settore_nmin= df_g50[df_g50.n_karst >= n_karst_soglia].sort_values(\"n_karst\").sector2000_50.tolist()[0]\n",
    "    \n",
    "    # se non metto nulla negli attributi della def usa il max\n",
    "    if hh  == 'none':\n",
    "        h = settore_nmax\n",
    "        \n",
    "    elif df_g50[df_g50.sector2000_50==hh].n_karst.tolist()[0] < n_karst_soglia:\n",
    "        h = settore_nmin\n",
    "        \n",
    "    else:\n",
    "        h = hh\n",
    "    \n",
    "    # ricorda in df0 dbase e dave sono invertiti\n",
    "    zo_dave = df0[(df0.sector2000_50==h)&(df0.era == \"Giovane\")].dbase\n",
    "    # per aumentare la stima del numero considero anche quelli antichi\n",
    "    #zo_dave = df0[df0.sector2000_50==h].dbase\n",
    "\n",
    "    # TO DO TRY WITH st.truncexp\n",
    "    #params = st.expon.fit(zo_dave, floc=0)\n",
    "    #scale = params[1]/correzione_distr_exp\n",
    "    \n",
    "    \n",
    "    #====================================================================================\n",
    "    n_bins = int(zo_dave.max() - zo_dave.min())\n",
    "    y, x = np.histogram(zo_dave.values, bins=n_bins, density=True)\n",
    "    x = (x + np.roll(x, -1))[:-1] / 2.0  #per ogni bin (x + x2)/2 trovo xc centroid\n",
    "    x_inf = x[np.argmax(y)]\n",
    "    x_sup = x.max()\n",
    "    \n",
    "    data_trunc = zo_dave[zo_dave>x_inf].values\n",
    "    params = st.expon.fit(data_trunc, floc=0)\n",
    "    scale = round(params[-1],3)\n",
    "    \n",
    "    # area sotto le barre da x_inf -> x.max()\n",
    "    cdfdata = expon.cdf(x_sup, scale=scale) - expon.cdf(x_inf, scale=scale) \n",
    "    # area sotto le barre da 0 a x_inf  (quantità da stimare)\n",
    "    cdfinf = expon.cdf(x_inf, scale=scale) \n",
    "    # area oltre x.max()\n",
    "    cdfsup = 1 - expon.cdf(x_sup, scale=scale)\n",
    "\n",
    "    k_range = int(zo_dave[zo_dave>x_inf].shape[0])\n",
    "    k_inf = int(k_range/cdfdata*cdfinf)\n",
    "    k_sup = int(k_range/cdfdata*cdfsup)\n",
    "    \n",
    "    k_tot_pop = k_range + k_inf + k_sup\n",
    "    \n",
    "    \n",
    "    #===========================================================================================================\n",
    "    \n",
    "\n",
    "    zo_perc5 = int(np.percentile(zo_dave, 5))\n",
    "    zo_perc95 = int(np.percentile(zo_dave, 95))\n",
    "    zo_karstnum = zo_dave.shape[0]\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # calcolo le proporzioni tra i karst sotto il 5p\n",
    "    # 1. zo_karstnum_simul è il numero su 1000 dei karst individuati a rilievo\n",
    "    # 2. zo_nbelow5p_simul è la stima su 1000 dei karst non individuati\n",
    "    # zo_tot è il totale della popolazione (tolto il frattile superiore al 95% che viene considerato un errore)\n",
    "    # zo_tot_prop è la proporzione tra il totale della simulazione e 1+2\n",
    "    # zo_pop_prop è la proporzione tra il totale della popolazione (compreso frattile >95%) e 1+2\n",
    "\n",
    "    \n",
    "    r = expon.rvs(scale=scale, size=size)\n",
    "\n",
    "    zo_karstnum_simul = r[(zo_perc5<r)&(r<zo_perc95)].shape[0]\n",
    "    zo_nbelow5p_simul = r[r<zo_perc5].shape[0]\n",
    "    zo_tot = zo_karstnum_simul + zo_nbelow5p_simul\n",
    "    zo_tot_prop = zo_tot/zo_karstnum_simul\n",
    "    zo_pop_prop = size/zo_karstnum_simul\n",
    "\n",
    "    zo_karstnum_simul, zo_nbelow5p_simul, zo_tot, zo_tot_prop, zo_pop_prop\n",
    "\n",
    "    zo_nbelow5p = int(round(zo_karstnum/zo_karstnum_simul*zo_nbelow5p_simul,0))\n",
    "    zo_nbelow5p\n",
    "\n",
    "    # the number of data to obtain 27 karst less than 6m is\n",
    "    # 37/538*1000\n",
    "\n",
    "    zo_karstnum_pop = int(round(zo_karstnum/zo_karstnum_simul*r.shape[0],0))\n",
    "    zo_karstnum_pop\n",
    "\n",
    "    # zo_karstnum_pop = sono quelli totali della simulazione\n",
    "    # zo_karstnum = sono quelli effettivi individuati 95p e non individuati)\n",
    "\n",
    "    zo_karstnum_pop = int(round(zo_pop_prop *zo_karstnum, 0))\n",
    "    \n",
    "    # k_inf, k_tot_pop, k_range, k_sup\n",
    "    # sono gli indici nuovi aggiunti con il nuovo approccio\n",
    "    \n",
    "    return zo_nbelow5p, zo_karstnum_pop, zo_karstnum, settore_nmax, scale, k_inf, k_tot_pop, k_range, k_sup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calcolo diametro per ogni strato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dia_diretto(row, t, df_prop, df_prop_rock, stochastic = False):\n",
    "    \n",
    "    '''\n",
    "    input:\n",
    "    row del df_zo\n",
    "    t = int(vita utile dell'opera serve per calcolare la dissoluzione e il dia finale)\n",
    "    df_prop = dataframe con le proprietà per ogni IG\n",
    "    df_prop_rock = dataframe con le proprietpà del substrato\n",
    "    stochastic = False use deterministic values True use uniform distribution MIN-MAX\n",
    "    return:\n",
    "    d_top : float (diametro parte alta dello strato)\n",
    "    \n",
    "    versioni:\n",
    "    1° deterministico, ogni d_0 crea un d_top, e Pds se permiano e quat = 0 d_top = d_0\n",
    "\n",
    "    2° prob usa MIN-MAX, d_top = 0 se h_perm > 40 o se h_perm+h_quat >50\n",
    "    \n",
    "    3°  considerazioni circa il Pds e la formazione di inbuti o subsidenze\n",
    "        si fanno a posteriori\n",
    "        \n",
    "    4° Nota assunzione df['h_perm'] = df_orig['h_perm'] + df_orig['h_giur'] + df_orig['h_marne'] --> unità 17\n",
    "        scorporare il h_perm --> unità 17\n",
    "                      h_giur --> unità 16\n",
    "                      h_marne --> unità 14\n",
    "    '''\n",
    "    \n",
    "    d_0 = row.dave\n",
    "    \n",
    "    lista_argille = df_prop[df_prop.lithotipo == 'a'].index.tolist()\n",
    "    lista_sabbie = df_prop[df_prop.lithotipo == 's'].index.tolist()\n",
    "    \n",
    "    \n",
    "    #==========================================================================================\n",
    "    #                   QUI CREO LA LISTA CON GLI IGE IN QUEL KARST SPECIFICO\n",
    "    #           PUO' ESSERE MESSA COME DATO DI INPUT SE INTERPOLATA ESTERNAMENTE\n",
    "    #   OPPURE PUO ESSERE CREATA QUI DI SEGUITO PER INTERPOLAZIONE CON I BOREHOLES FITTIZI\n",
    "    \n",
    "    \n",
    "    if row.h_perm>0:\n",
    "        list_layer = ['17']\n",
    "        list_lathick = [row.h_perm]\n",
    "    else:\n",
    "        list_layer = []\n",
    "        list_lathick  =[]\n",
    "\n",
    "    if row.h_quat < 5:\n",
    "        list_layer.append(row.IGE)\n",
    "        list_lathick.append(row.h_quat)\n",
    "    else:\n",
    "        # se la litologia principale è argilla\n",
    "        if df_prop.loc[row.IGE, 'lithotipo'] == 'a':\n",
    "            # DO IT\n",
    "            # crea una funzione che interpone un litotipo in sabbia\n",
    "            # di spessore random uniform da 1/10 a 1/5 dello spessore totale dell'argilla\n",
    "            list_layer.append(row.IGE)\n",
    "            #list_layer.append(random.choice(lista_sabbie))\n",
    "            list_layer.append(row.IGE_sec)\n",
    "            list_layer.append(row.IGE)\n",
    "            ige_perc = row.IGE_SEC_PERC\n",
    "            randthick = random.uniform(row.h_quat*ige_perc*1.2, row.h_quat*ige_perc*0.8)\n",
    "            list_lathick.append((row.h_quat - randthick)/2)\n",
    "            list_lathick.append(randthick)\n",
    "            list_lathick.append((row.h_quat - randthick)/2)\n",
    "        else:  \n",
    "            # se la litologia principale è sabbia\n",
    "            # DO IT\n",
    "            # crea una funzione che interpone un litotipo in argilla\n",
    "            # di spessore random uniform da 1/10 a 1/5 dello spessore totale della sabbia  \n",
    "            list_layer.append(row.IGE)\n",
    "            list_layer.append(random.choice(lista_argille))\n",
    "            list_layer.append(row.IGE)\n",
    "            randthick = random.uniform(row.h_quat/10, row.h_quat/5)\n",
    "            list_lathick.append((row.h_quat - randthick)/2)\n",
    "            list_lathick.append(randthick)\n",
    "            list_lathick.append((row.h_quat - randthick)/2)\n",
    "    \n",
    "    list_sv = []\n",
    "    sv_temp = 0\n",
    "    # invertire layer e calc sv\n",
    "    for i in range(len(list_layer)):\n",
    "        sv_temp += df_prop.gamma[i] * list_lathick[-1*(1+i)]\n",
    "        list_sv.append(sv_temp)\n",
    "    \n",
    "    # come per list_layer ordino dal basso verso l'alto\n",
    "    list_sv.reverse()\n",
    "\n",
    "    # a questo punto ho la lista con litotipi e spessori\n",
    "    # per le rocce usa solo calcari ma tieni in conto una futura revisione\n",
    "    \n",
    "    #===================================================================================================\n",
    "\n",
    "    nloop = 0   # nloop inizializzato a zero\n",
    "\n",
    "    #=================================================   inizio calcdia_strato   =================================================\n",
    "\n",
    "    def calcdia_strato(df_prop, d_pod, layer_i, thick_i, nloop, sv_i):\n",
    "        # creare una funzione che restituisce il diametro strato superiore        \n",
    "        # return i parametri che si aggiornano: d_top e sv \n",
    "\n",
    "        primoloop = True if nloop == 0 else False\n",
    "\n",
    "        _phi = df_prop.phi[layer_i]\n",
    "        if stochastic: _phi = random.uniform(df_prop.phi_MIN[layer_i], df_prop.phi_MAX[layer_i])\n",
    "        # nel caso in cui min max non siano definiti\n",
    "        if _phi == 0: _phi = df_prop.phi[layer_i]\n",
    "        radphi = np.radians(_phi)\n",
    "\n",
    "        _gamma = df_prop.gamma[layer_i]\n",
    "        if stochastic: _gamma = random.uniform(df_prop.gamma_MIN[layer_i], df_prop.gamma_MAX[layer_i])\n",
    "        # nel caso in cui min max non siano definiti\n",
    "        if _gamma == 0: _gamma = df_prop.gamma[layer_i]\n",
    "            \n",
    "        z1 = d_pod/2 * (1+2*np.sin(radphi))/(2*np.tan(radphi)*np.sin(radphi))\n",
    "        x_2 = d_pod/2 * (1 + np.sin(radphi))**2/(4*np.sin(radphi)**3)\n",
    "        X_2 = x_2 * 2\n",
    "        z2 = z1 + d_pod/2*(1+np.sin(radphi))/(4*np.tan(radphi)*np.sin(radphi)**2)\n",
    "        z3 = z2 + x_2/(np.tan(radphi))\n",
    "\n",
    "        zone = 1 if(thick_i < z2) else (2 if thick_i < z3 else 3)   \n",
    "\n",
    "        #sv = sv + df_prop.gamma[layer_i] * thick_i\n",
    "        nloop += 1 # per indicare loop successivi al primo\n",
    "\n",
    "        eta = 0\n",
    "\n",
    "        if df_prop.lithotipo[layer_i] == 'a':\n",
    "\n",
    "            '''\n",
    "            ********* ARGILLE  *******\n",
    "            '''\n",
    "\n",
    "            _IL = float(df_prop.IL[layer_i])\n",
    "            # ci sono casi in cui il IL risulta negativo\n",
    "            if _IL <0 : _IL = 0.05 \n",
    "\n",
    "\n",
    "            _c = df_prop.c[layer_i]              \n",
    "            if stochastic: _c = random.uniform(df_prop.c_MIN[layer_i], df_prop.c_MAX[layer_i])\n",
    "            # nel caso in cui min max non siano definiti\n",
    "            if _c == 0: df_prop.c[layer_i]\n",
    "            # se c è effettivamente prossimo a zero metter un valore minimo\n",
    "            if _c == 0: _c = 1\n",
    "\n",
    "\n",
    "            _dfis = df_prop.Dfis[layer_i]\n",
    "            if stochastic: _dfis = random.uniform(df_prop.Dfis_MIN[layer_i], df_prop.Dfis_MAX[layer_i])\n",
    "            # nel caso in cui min max non siano definiti\n",
    "            if _dfis == 0: df_prop.Dfis[layer_i]\n",
    "\n",
    "\n",
    "            dcrit = 2 * thick_i * ((np.tan(np.pi/4-radphi/2))**2 * np.tan(radphi) + 2 *_c * (1 - _IL)/sv_i)\n",
    "            if dcrit < 0.01: dcrit = 0.01\n",
    "            dpipe = np.exp(_IL) * _dfis * _gamma * thick_i / _c\n",
    "\n",
    "            # parametro per il calcolo Pds\n",
    "            eta = thick_i * dpipe / dcrit\n",
    "\n",
    "            ## primo loop\n",
    "\n",
    "            d_top = d_pod\n",
    "\n",
    "            if (not primoloop) and (d_pod > dpipe): \n",
    "                if d_pod > dcrit:\n",
    "                    d_top = dcrit\n",
    "                else:\n",
    "                    d_top = dpipe\n",
    "            else:\n",
    "                #per il primo loop solamente o se d_pod < dpipe\n",
    "                #d_top = d_pod   ********** ATTENZIONE ************\n",
    "                d_top = d_pod\n",
    "                pass\n",
    "\n",
    "        else:\n",
    "            '''\n",
    "            *********  SABBIE   *******\n",
    "            '''\n",
    "\n",
    "            if zone == 1 :\n",
    "                d_top = (X_2 - d_pod) / z2 * thick_i + d_pod\n",
    "                ##print('zone1 :',zone,'z2',z2,'z3',z3,'dpod: ',d_pod, 'phi: ', df_prop.phi[layer_i], 'radphi: ', radphi, 'gamma: ', _gamma)\n",
    "            elif zone == 2 :\n",
    "                d_top = X_2\n",
    "                ##print('zone2 :',zone,'z2',z2,'z3',z3,'dpod: ',d_pod, 'phi: ', df_prop.phi[layer_i], 'radphi: ', radphi, 'gamma: ', _gamma)\n",
    "#                 elif thick_i > (z3*3):\n",
    "#                     #print(\"thick_i > (z3*3)\")\n",
    "#                     d_top = 0\n",
    "            else :\n",
    "                #d_top = 0    ********** ATTENZIONE ************\n",
    "                d_top = X_2\n",
    "                ##print('zone3 :',zone,'z2',z2,'z3',z3,'dpod: ',d_pod, 'phi: ', df_prop.phi[layer_i], 'radphi: ', radphi, 'gamma: ', _gamma)\n",
    "\n",
    "        vol = np.pi * thick_i / 3 * (d_pod**2 / 4 + d_pod * d_top / 4 + d_top**2 / 4)\n",
    "\n",
    "        if zone == 3: vol = vol + np.pi * d_pod**2 * (thick_i - z3) / 4\n",
    "\n",
    "        e_i = float(df_prop.e[layer_i])\n",
    "        ecr_i = float(df_prop[\"e,cr\"][layer_i])\n",
    "\n",
    "        deltav = (ecr_i - e_i) /(1 + ecr_i) * vol\n",
    "\n",
    "        return d_top, nloop, vol, eta\n",
    "\n",
    "    #===========================================  end calcdia_strato      ============================================================  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #================================================================================================================================        \n",
    "\n",
    "    def calc_fixed_par(t, df_prop_rock, d_0, list_layer):\n",
    "\n",
    "        # when doing back calculation use t=0\n",
    "\n",
    "        v_r = df_prop_rock.v_r.Calcari\n",
    "        h_p = d_0/2.5  # da controllare\n",
    "        D1 = v_r * t + d_0\n",
    "        m_k = df_prop_rock.m_k.Calcari\n",
    "        k_k = df_prop_rock.k_k.Calcari\n",
    "        phiradmedio = df_prop.phi[list_layer].mean()\n",
    "\n",
    "        return(v_r, h_p, D1, m_k, k_k, phiradmedio)\n",
    "\n",
    "    #===========================================   end  calc_fixed_par       =======================================================  \n",
    "\n",
    "\n",
    "    # sovrascrivo d_0 a quello originale in teoria dovrebbe chiamarsi D1 ovverso il diametro d_0 allargato con dissoluzione a t anni\n",
    "\n",
    "    v_r, h_p, d_0, m_k, k_k, phiradmedio  = calc_fixed_par(t, df_prop_rock, d_0, list_layer)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #===========================================   inizio calcolo       =======================================================\n",
    "\n",
    "\n",
    "    d_top = d_0\n",
    "\n",
    "    list_dia = [d_0] # il primo della lista è il d_pod\n",
    "    list_vol = []\n",
    "    list_eta = []\n",
    "\n",
    "    for i, e in enumerate(list_layer):\n",
    "        # calcola per tutti i layer quando esce dal loop d_0 = diam al piano campagna\n",
    "        layer_i = e\n",
    "        thick_i = list_lathick[i]\n",
    "        sv_i = list_sv[i]\n",
    "        \n",
    "        d_top, nloop, vol, eta = calcdia_strato(df_prop, d_top, layer_i, thick_i, nloop, sv_i)\n",
    "\n",
    "        list_dia.append(d_top)\n",
    "        list_vol.append(vol)\n",
    "        list_eta.append(eta)\n",
    "\n",
    "    # d_0 è il d alla base (INPUT)\n",
    "    # d_top è quello calcolato a piano campagna (OUTPUT)\n",
    "\n",
    "\n",
    "    #===========================================   fine ciclo iterativo       =======================================================\n",
    "\n",
    "\n",
    "\n",
    "    #===========================================    inizio calc subsidenze    =======================================================\n",
    "    # TODO: aggiungere un check if list_dia[-2] == 0 allora prendere il precedente [-3] ecc...    \n",
    "    \n",
    "    \n",
    "    dpod1 = list_dia[-2] # _pod che genera subsidenza devo prendere la penultima perché l'ultima è in superficie\n",
    "    sv1 = list_sv[-1]    # sv' al primo strato in alto (ultimo nella lista)\n",
    "    lay1 = list_layer[-1] # layer del primo strato ovvero l'IGE principale\n",
    "    th1  = list_lathick[-1] # thickness del primo strato superficiale (ultimo nella lista)\n",
    "    \n",
    "    d_sub = 2 * np.sqrt(th1**2 - (dpod1/2)**2)\n",
    "    # per spessori di strato modesti gestisco radici negative \n",
    "    if (th1**2 - (dpod1/2)**2)<0: d_sub = dpod1\n",
    "           \n",
    "    h_sub = sv1 * dpod1**3 * th1 * (1-df_prop.nu[lay1]**2) * (1+((df_prop.nu[lay1])/(1-df_prop.nu[lay1]))\n",
    "                                                             ) / (df_prop.E[lay1] * 1000 * ((4 * dpod1**2) - th1**2))\n",
    "    # per evitare soluzioni negative \n",
    "    # associare alla subsidenza un numero alto, si associerà anziche una subsidenza un inbuto\n",
    "    if ((4 * dpod1**2) - th1**2) <= 0: h_sub = 100\n",
    "    \n",
    "    \n",
    "        \n",
    "    #===========================================    inizio calcprob         =========================================================    \n",
    "\n",
    "    def calc_prob(df_prop, df_prop_rock, list_layer, d_top, v_r, h_p, D1, m_k, k_k, phiradmedio, t, list_vol, list_eta):\n",
    "\n",
    "        # H_k = non richiesta\n",
    "\n",
    "        Va_0 = np.pi * D1**2 * h_p/4\n",
    "\n",
    "        Va_1 = np.pi * m_k * (k_k + v_r * t / m_k) * (3*D1**2 + ( 2 * m_k * (3*D1 + \\\n",
    "                2 * m_k / np.tan(phiradmedio)) / (np.tan(phiradmedio))))/12\n",
    "\n",
    "        Va = Va_0 + Va_1\n",
    "\n",
    "        list_Va0i = list_vol + Va_0\n",
    "\n",
    "        sum_Va0i = np.array(list_Va0i).sum()\n",
    "        sum_eta = np.array(list_eta).sum()\n",
    "        sum_m = np.array(list_lathick).sum()\n",
    "\n",
    "        Pds = 1 - np.exp(-Va / sum_Va0i * sum_eta / sum_m)\n",
    "\n",
    "        return Pds\n",
    "\n",
    "    #===========================================     fine  calcprob         ========================================================= \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Aggiungere che se la Pds < 0.3 si forma una sinclinale con diametro da calcolare\n",
    "\n",
    "    Pds = calc_prob(df_prop, df_prop_rock, list_layer, d_top, v_r, h_p, d_0, m_k, k_k, phiradmedio, t, list_vol, list_eta)\n",
    "    #print(\"Pds is :\", Pds, \"d_top is: \", d_top)\n",
    "\n",
    "    #  IL CALCOLO IN FUNZIONE DI PDS LO FACCIO MANUALMENTE DOPO\n",
    "    #         if ((Pds < 0.1) & (row.h_perm <20)): \n",
    "    #             d_top = 0\n",
    "    #             #print('(Pds < 0.3) & (row.h_perm <20)')\n",
    "    #         elif ((Pds < 0.3) & (row.h_perm >=20)):\n",
    "    #             d_top = 0\n",
    "    #             #print('(Pds < 0.5) & (row.h_perm >20)')\n",
    "    #         else:\n",
    "    #             pass\n",
    "            \n",
    "\n",
    "    \n",
    "    ##print(\"\\n\", \"===========================================================================================================\", \"\\n\")\n",
    "\n",
    "    return pd.Series([d_top, Pds, d_sub, h_sub])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREO UN QUADRATO GRANDE ATTORNO ALL'AREA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_zo(df_g50, h, zo_nbelow5p, zo_karstnum, zo_karstnum_pop, df_pred, scale):\n",
    "    # per i diametri\n",
    "    # devo fare una distribuzione con 68 elementi e inserire solo i primi \n",
    "    # 27 elementi ordinati per grandezza\n",
    "\n",
    "    xmin = min(df_g50.loc[df_g50.index[h],'vtrackdown2000x'],\n",
    "                               df_g50.loc[df_g50.index[h+1],'vtrackdown2000x'],\n",
    "                               df_g50.loc[df_g50.index[h],'vtrackup2000x'],\n",
    "                               df_g50.loc[df_g50.index[h+1],'vtrackup2000x']\n",
    "                              )\n",
    "    xmax = max(df_g50.loc[df_g50.index[h],'vtrackdown2000x'],\n",
    "                               df_g50.loc[df_g50.index[h+1],'vtrackdown2000x'],\n",
    "                               df_g50.loc[df_g50.index[h],'vtrackup2000x'],\n",
    "                               df_g50.loc[df_g50.index[h+1],'vtrackup2000x']\n",
    "                              )\n",
    "    ymin = min(df_g50.loc[df_g50.index[h],'vtrackdown2000y'],\n",
    "                               df_g50.loc[df_g50.index[h+1],'vtrackdown2000y'],\n",
    "                               df_g50.loc[df_g50.index[h],'vtrackup2000y'],\n",
    "                               df_g50.loc[df_g50.index[h+1],'vtrackup2000y']\n",
    "                              )\n",
    "    ymax = max(df_g50.loc[df_g50.index[h],'vtrackdown2000y'],\n",
    "                               df_g50.loc[df_g50.index[h+1],'vtrackdown2000y'],\n",
    "                               df_g50.loc[df_g50.index[h],'vtrackup2000y'],\n",
    "                               df_g50.loc[df_g50.index[h+1],'vtrackup2000y']\n",
    "                              )\n",
    "    area_gr = (xmax-xmin)*(ymax-ymin)/1e6\n",
    "    n_karst_5p_areag = int(round(zo_nbelow5p/df_g50.sector_area[h]*area_gr,0))\n",
    "    # 5p + visiblekarst\n",
    "    n_karst_areag = int(round(zo_nbelow5p + zo_karstnum / df_g50.sector_area[h] * area_gr,0))\n",
    "    # 5p + visiblekarst + outofsize\n",
    "    zo_karstnum_pop_areag = int(round(zo_karstnum_pop / df_g50.sector_area[h] * area_gr,0))\n",
    "\n",
    "    # n_karst_5p_areag, n_karst_areag, zo_karstnum_pop_areag\n",
    "\n",
    "    stoch_karstx = np.random.uniform(xmin, xmax, n_karst_areag)\n",
    "    stoch_karsty = np.random.uniform(ymin, ymax, n_karst_areag)\n",
    "    # creo the full curve in the areagrande areag estraggo i primi n-karst-5p-areag valori\n",
    "    stoch_karstdia = expon.rvs(scale=scale, size=zo_karstnum_pop_areag)\n",
    "    stoch_karstdia.sort()\n",
    "    stoch_karstdia = stoch_karstdia[:n_karst_areag]\n",
    "\n",
    "    #### creo l'array con tutti i karst sotchastici nell'area grande\n",
    "\n",
    "    zo_arr = np.vstack([stoch_karstx, stoch_karsty, stoch_karstdia]).T\n",
    "\n",
    "    #### Rimuovo i karst fuori dall'area di settore\n",
    "\n",
    "    def abs_sum(row, i):\n",
    "        check = False\n",
    "        polygon = Polygon([tuple(df_g50[['vtrackup2000x','vtrackup2000y']].loc[i,:]),\n",
    "                      tuple(df_g50[['vtrackup2000x','vtrackup2000y']].loc[i+1,:]),\n",
    "                      tuple(df_g50[['vtrackdown2000x','vtrackdown2000y']].loc[i+1,:]),\n",
    "                      tuple(df_g50[['vtrackdown2000x','vtrackdown2000y']].loc[i,:])])\n",
    "\n",
    "        point = Point(row[0], row[1])\n",
    "\n",
    "        if polygon.contains(point): check = True\n",
    "\n",
    "        return check\n",
    "\n",
    "    zo_arr = np.vstack([zo_arr.T, np.apply_along_axis(abs_sum, 1, zo_arr, h)]).T\n",
    "    zo_arr = zo_arr[zo_arr[:,3]==True][:,:3]\n",
    "    df_zo = pd.DataFrame(zo_arr, columns=['long','lat','dave'])\n",
    "    # rinomino le colonne \n",
    "    # zo_arr = np.vstack([stoch_karstx, stoch_karsty, stoch_karstdia])\n",
    "    # \"dave\" qui è il d alla base attenzione ho invertito nel df0 dave è al piano campagna\n",
    "\n",
    "    df_zo['sector2000_50'] = int(h)           # per associazione con df0 e df_g50\n",
    "    df_zo['sector2000_50_str'] = str(h)     # per associazione con df0\n",
    "    \n",
    "    points = df_pred.loc[:,['POINT_X','POINT_Y']]\n",
    "    values_h_perm = df_pred.loc[:,['h_perm']]\n",
    "    values_h_quat = df_pred.loc[:,['h_quat']]\n",
    "    values_ige = df_pred.loc[:,['IGE']]\n",
    "    values_ige_sec = df_pred.loc[:,['IGE_sec']]\n",
    "    values_ige_sec_perc = df_pred.loc[:,['IGE_SEC_PERC']]\n",
    "    values_tipo = df_pred.loc[:,['Tipo']]\n",
    "    values_h_rilevato = df_pred.loc[:,['h_rilevato']]\n",
    "\n",
    "    point_pred = df_zo.loc[:,['long','lat']]\n",
    "    df_zo['h_perm'] = predict(points, values_h_perm, point_pred)\n",
    "    df_zo['h_quat'] = predict(points, values_h_quat, point_pred)\n",
    "    df_zo['IGE'] = predict(points, values_ige, point_pred)\n",
    "    df_zo['IGE_sec'] = predict(points, values_ige_sec, point_pred)\n",
    "    df_zo['IGE_SEC_PERC'] = predict(points, values_ige_sec_perc, point_pred)\n",
    "    df_zo['Tipo'] = predict(points, values_tipo, point_pred)\n",
    "    df_zo['h_rilevato'] = predict(points, values_h_rilevato, point_pred)\n",
    "    \n",
    "\n",
    "\n",
    "    #### Calcolo diametro in superficie\n",
    "    \n",
    "    df_zo[['dbase','Pds', 'd_sub', 'h_sub']] = df_zo.apply(calculate_dia_diretto, axis=1, args=(100, df_prop, df_prop_rock, True))\n",
    "    df_zo['dbase'] = df_zo['dbase'].astype('float', errors='ignore')\n",
    "    \n",
    "    def create_df_zo_100(row, df_g50, df_zo):\n",
    "\n",
    "        for i in range(df_g50.shape[0]-1):\n",
    "\n",
    "            check = False\n",
    "\n",
    "            polygon = Polygon([tuple(df_g50[['vtrackupx','vtrackupy']].loc[i,:]),\n",
    "                          tuple(df_g50[['vtrackupx','vtrackupy']].loc[i+1,:]),\n",
    "                          tuple(df_g50[['vtrackdownx','vtrackdowny']].loc[i+1,:]),\n",
    "                          tuple(df_g50[['vtrackdownx','vtrackdowny']].loc[i,:])])\n",
    "\n",
    "            point = Point(row['long'], row['lat'])\n",
    "\n",
    "            if polygon.contains(point): \n",
    "                check = True        \n",
    "                break\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        return check\n",
    "    \n",
    "    df_zo['check_100'] = df_zo.apply(create_df_zo_100, axis=1, args = (df_g50, df_zo))\n",
    "    \n",
    "    return df_zo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applico la def \"create_df_zo\" per creare il dataframe con la colonna dbase --> il diametro in superficie  --> una sola iterazione stochastica\n",
    "#### Risultati non utizzati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zo = pd.DataFrame(columns=['long', 'lat', 'dave', 'sector2000_50', 'sector2000_50_str', 'h_perm',\n",
    "       'h_quat', 'IGE', 'IGE_sec', 'IGE_SEC_PERC', 'dbase','Tipo','h_rilevato','check_100'])\n",
    "\n",
    "#zo_nbelow5p, zo_karstnum_pop, zo_karstnum, settore_nmax, scale = trova_distr_exp(df_g50)\n",
    "\n",
    "\n",
    "##109\n",
    "## pkfinale = 40\n",
    "for hh in range (int(1000 / (div_settori * m_punti) * pkfinale)):\n",
    "    \n",
    "    zo_nbelow5p, zo_karstnum_pop, zo_karstnum, settore_nmax, scale, k_inf, k_tot_pop, k_range, k_sup = trova_distr_exp(df_g50, hh==hh)\n",
    "    df_zo = df_zo.append(create_df_zo(df_g50, hh, zo_nbelow5p, zo_karstnum, zo_karstnum_pop, df_pred, scale), ignore_index=True, sort=True)\n",
    "    \n",
    "# associo le tipologie di dato corrette\n",
    "\n",
    "df_zo['dbase'] = df_zo['dbase'].astype('float')\n",
    "df_zo['sector2000_50'] = df_zo['sector2000_50'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zo[df_zo.dbase>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creo file df_zo  con iterazioni stochastiche per mappe\n",
    "## Creo file df_zo_100 ma con iterazioni stochastiche e solo nell'intorno 100m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Funzioni propedeutiche al calcolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rilevato_ksmall(row):\n",
    "    \n",
    "    if (row['Tipo'] == 'Rilevato') & (row['dbase'] < soglia_dia_karst):\n",
    "        quantity = 1     \n",
    "    else:\n",
    "        quantity = 0\n",
    "    return quantity\n",
    "\n",
    "\n",
    "def rilevato_klarge(row):\n",
    "    \n",
    "    if (row['Tipo'] == 'Rilevato') & (row['dbase'] >= soglia_dia_karst): \n",
    "        quantity = 1\n",
    "    else:\n",
    "        quantity = 0    \n",
    "    return quantity\n",
    "\n",
    "def trincea_ksmall(row):\n",
    "    \n",
    "    if (row['Tipo'] == 'Trincea') & (row['dbase'] < soglia_dia_karst):\n",
    "        quantity = 1     \n",
    "    else:\n",
    "        quantity = 0\n",
    "    return quantity\n",
    "\n",
    "\n",
    "def trincea_klarge(row):\n",
    "    \n",
    "    if (row['Tipo'] == 'Trincea') & (row['dbase'] >= soglia_dia_karst): \n",
    "        quantity = 1\n",
    "    else:\n",
    "        quantity = 0    \n",
    "    return quantity\n",
    "\n",
    "\n",
    "def vidotto_ksmall(row):\n",
    "    \n",
    "    if (row['Tipo'] != 'Rilevato') & (row['Tipo'] != 'Trincea') & (row['dbase'] < soglia_dia_karst) & (row['h_rilevato'] > lim_via_bridge):        \n",
    "        quantity = 1        \n",
    "    else:        \n",
    "        quantity = 0        \n",
    "    return quantity\n",
    "\n",
    "\n",
    "def vidotto_klarge(row):\n",
    "    \n",
    "    if (row['Tipo'] != 'Rilevato') & (row['Tipo'] != 'Trincea') & (row['dbase'] >= soglia_dia_karst) & (row['h_rilevato'] > lim_via_bridge):        \n",
    "        quantity = 1        \n",
    "    else:        \n",
    "        quantity = 0        \n",
    "    return quantity\n",
    "\n",
    "\n",
    "def passaggio_ksmall(row):\n",
    "    \n",
    "    if (row['Tipo'] != 'Rilevato') & (row['Tipo'] != 'Trincea') & (row['dbase'] < soglia_dia_karst) & (row['h_rilevato'] <= lim_via_bridge):        \n",
    "        quantity = 1        \n",
    "    else:        \n",
    "        quantity = 0        \n",
    "    return quantity\n",
    "\n",
    "\n",
    "def passaggio_klarge(row):\n",
    "    \n",
    "    if (row['Tipo'] != 'Rilevato') & (row['Tipo'] != 'Trincea') & (row['dbase'] >= soglia_dia_karst) & (row['h_rilevato'] <= lim_via_bridge):        \n",
    "        quantity = 1        \n",
    "    else:        \n",
    "        quantity = 0        \n",
    "    return quantity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### INIZIO IL CALCOLO STOCHASTICO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inizializzo df_zo\n",
    "\n",
    "df_zo = pd.DataFrame(columns=['long', 'lat', 'dave', 'sector2000_50', 'sector2000_50_str', 'h_perm',\n",
    "       'h_quat', 'IGE', 'IGE_sec', 'IGE_SEC_PERC', 'dbase', 'Tipo', 'h_rilevato', 'Pds', 'check_100', 'epoch'])\n",
    "\n",
    "# se voglio una simulazione con la distribuzione spaziale più sfigata\n",
    "#zo_nbelow5p, zo_karstnum_pop, zo_karstnum, settore_nmax, scale = trova_distr_exp(df_g50, hh='none')\n",
    "\n",
    "#======================================================================================================\n",
    "\n",
    "for epoch in trange(epochs, file=sys.stdout, desc='epoch advance'):\n",
    "    \n",
    "    for hh in range (int(1000 / (div_settori * m_punti) * pkfinale)):\n",
    "        \n",
    "        zo_nbelow5p, zo_karstnum_pop, zo_karstnum, settore_nmax, scale, k_inf, k_tot_pop, k_range, k_sup = trova_distr_exp(df_g50, hh=hh)\n",
    "        \n",
    "        row_temp = create_df_zo(df_g50, hh, zo_nbelow5p, zo_karstnum, zo_karstnum_pop, df_pred, scale)\n",
    "        row_temp['epoch'] = epoch\n",
    "        \n",
    "        # sort=True --> to avoid warning appearing\n",
    "        df_zo = df_zo.append(row_temp, ignore_index=True, sort=False)\n",
    "        \n",
    "#======================================================================================================\n",
    "    \n",
    "# nel df_zo_100 devono rimanere i punti che stanno nella fascia 100m e che hanno dia > 0\n",
    "df_zo_100 = df_zo[df_zo.check_100 == True]\n",
    "df_zo_100 = df_zo_100[df_zo_100.dbase > 0]\n",
    "\n",
    "# creo colonne per la definizione delle quantita di interventi\n",
    "try:\n",
    "    df_zo_100['ril_ksmall'] = df_zo_100.apply(rilevato_ksmall, axis=1)\n",
    "    df_zo_100['ril_klarge'] = df_zo_100.apply(rilevato_klarge, axis=1)\n",
    "    df_zo_100['tri_ksmall'] = df_zo_100.apply(trincea_ksmall, axis=1)\n",
    "    df_zo_100['tri_klarge'] = df_zo_100.apply(trincea_klarge, axis=1)\n",
    "    df_zo_100['via_ksmall'] = df_zo_100.apply(vidotto_ksmall, axis=1)\n",
    "    df_zo_100['via_klarge'] = df_zo_100.apply(vidotto_klarge, axis=1)\n",
    "    df_zo_100['pas_ksmall'] = df_zo_100.apply(passaggio_ksmall, axis=1)\n",
    "    df_zo_100['pas_klarge'] = df_zo_100.apply(passaggio_klarge, axis=1)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# area della semisfera\n",
    "\n",
    "df_zo['vol_dave'] = 4/3*np.pi*(df_zo.dave/2)**3 / 2\n",
    "df_zo_100['vol_dave'] = 4/3*np.pi*(df_zo_100.dave/2)**3 / 2\n",
    "df_zo['Litho'] = df_prop.lithotipo[df_zo.IGE].tolist()\n",
    "\n",
    "df_zo.fillna(0, inplace=True)\n",
    "df_zo_100.fillna(0, inplace=True)\n",
    "\n",
    "df_zo['h_copertura'] = df_zo.h_quat + df_zo.h_perm\n",
    "df_zo_100['h_copertura'] = df_zo.h_quat + df_zo.h_perm\n",
    "df_zo['dia_subimb']=df_zo.loc[:,['d_sub','dbase']].max(axis=1)\n",
    "    \n",
    "df_zo.to_csv('df_zo.csv', sep=';')\n",
    "df_zo_100.to_csv('df_zo_100.csv', sep=';')\n",
    "df.to_csv('df.csv', sep=';')\n",
    "df0.to_csv('df0.csv', sep=';')\n",
    "df.to_csv('df.csv', sep=';')\n",
    "df_g50.to_csv('df_g50.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## ==================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAKE df0 similar to df_zo mancano check_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###  gia fatto!!!  ##########\n",
    "# point_pred = df0.loc[:,['long','lat']]\n",
    "# df0['h_perm'] = predict(points, values_h_perm, point_pred)\n",
    "# df0['h_quat'] = predict(points, values_h_quat, point_pred)\n",
    "# df0['IGE'] = predict(points, values_ige, point_pred)\n",
    "# df0['Tipo'] = predict(points, values_tipo, point_pred)\n",
    "# df0['h_rilevato'] = predict(points, values_h_rilevato, point_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0_100 = df0[df0.check_100==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df0_100['ril_ksmall'] = df0_100.apply(rilevato_ksmall, axis=1)\n",
    "    df0_100['ril_klarge'] = df0_100.apply(rilevato_klarge, axis=1)\n",
    "    df0_100['tri_ksmall'] = df0_100.apply(trincea_ksmall, axis=1)\n",
    "    df0_100['tri_klarge'] = df0_100.apply(trincea_klarge, axis=1)\n",
    "    df0_100['via_ksmall'] = df0_100.apply(vidotto_ksmall, axis=1)\n",
    "    df0_100['via_klarge'] = df0_100.apply(vidotto_klarge, axis=1)\n",
    "    df0_100['pas_ksmall'] = df0_100.apply(passaggio_ksmall, axis=1)\n",
    "    df0_100['pas_klarge'] = df0_100.apply(passaggio_klarge, axis=1)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0_100['epoch'] = 100000\n",
    "df0_100['h_copertura'] = df0_100.h_quat + df0_100.h_perm\n",
    "df0_100['Pds'] = np.where(df0_100.type == 'Subsidenza', 0.09, 1)\n",
    "df0_100 = df0_100[df_zo_100.columns.tolist()]\n",
    "df0_100.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Completa df_g50 con i risultati dell'analisi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zo['stipo'] = np.where(((df_zo.Pds < 0.1)&(df_zo.Pds>=0.01)&(df_zo.h_sub <=2)), 'subsidenza', 'imbuto')\n",
    "df_zo['stipo'] = np.where(((df_zo.Pds < 0.01)|(df_zo.dbase == 0)),'none',df_zo.stipo)\n",
    "df_g50['s_dsub_min'] = df_zo[df_zo.stipo == 'subsidenza'][['d_sub','sector2000_50']].groupby('sector2000_50').min()\n",
    "df_g50['s_dsub_mean'] = df_zo[df_zo.stipo == 'subsidenza'][['d_sub','sector2000_50']].groupby('sector2000_50').mean()\n",
    "df_g50['s_dsub_q99'] = df_zo[df_zo.stipo == 'subsidenza'][['d_sub','sector2000_50']].groupby('sector2000_50').quantile(.99)\n",
    "df_g50['s_dsub_max'] = df_zo[df_zo.stipo == 'subsidenza'][['d_sub','sector2000_50']].groupby('sector2000_50').max()\n",
    "df_g50['s_dsub_count'] = df_zo[df_zo.stipo == 'subsidenza'][['d_sub','sector2000_50']].groupby('sector2000_50').count()\n",
    "df_g50['s_dbase_min'] = df_zo[df_zo.stipo == 'imbuto'][['dbase','sector2000_50']].groupby('sector2000_50').min()\n",
    "df_g50['s_dbase_mean'] = df_zo[df_zo.stipo == 'imbuto'][['dbase','sector2000_50']].groupby('sector2000_50').mean()\n",
    "df_g50['s_dbase_q99'] = df_zo[df_zo.stipo == 'imbuto'][['dbase','sector2000_50']].groupby('sector2000_50').quantile(.99)\n",
    "df_g50['s_dbase_max'] = df_zo[df_zo.stipo == 'imbuto'][['dbase','sector2000_50']].groupby('sector2000_50').max()\n",
    "df_g50['s_dbase_count'] = df_zo[df_zo.stipo == 'imbuto'][['dbase','sector2000_50']].groupby('sector2000_50').count()\n",
    "df_g50['s_dsub_min_100'] = df_zo[(df_zo.stipo == 'subsidenza')&(df_zo.check_100 == True)][['d_sub','sector2000_50']].groupby('sector2000_50').min()\n",
    "df_g50['s_dsub_mean_100'] = df_zo[(df_zo.stipo == 'subsidenza')&(df_zo.check_100 == True)][['d_sub','sector2000_50']].groupby('sector2000_50').mean()\n",
    "df_g50['s_dsub_q99_100'] = df_zo[(df_zo.stipo == 'subsidenza')&(df_zo.check_100 == True)][['d_sub','sector2000_50']].groupby('sector2000_50').quantile(.99)\n",
    "df_g50['s_dsub_max_100'] = df_zo[(df_zo.stipo == 'subsidenza')&(df_zo.check_100 == True)][['d_sub','sector2000_50']].groupby('sector2000_50').max()\n",
    "df_g50['s_dsub_count_100'] = df_zo[(df_zo.stipo == 'subsidenza')&(df_zo.check_100 == True)][['d_sub','sector2000_50']].groupby('sector2000_50').count()\n",
    "df_g50['s_dbase_min_100'] = df_zo[(df_zo.stipo == 'imbuto')&(df_zo.check_100 == True)][['dbase','sector2000_50']].groupby('sector2000_50').min()\n",
    "df_g50['s_dbase_mean_100'] = df_zo[(df_zo.stipo == 'imbuto')&(df_zo.check_100 == True)][['dbase','sector2000_50']].groupby('sector2000_50').mean()\n",
    "df_g50['s_dbase_q99_100'] = df_zo[(df_zo.stipo == 'imbuto')&(df_zo.check_100 == True)][['dbase','sector2000_50']].groupby('sector2000_50').quantile(.99)\n",
    "df_g50['s_dbase_max_100'] = df_zo[(df_zo.stipo == 'imbuto')&(df_zo.check_100 == True)][['dbase','sector2000_50']].groupby('sector2000_50').max()\n",
    "df_g50['s_dbase_count_100'] = df_zo[(df_zo.stipo == 'imbuto')&(df_zo.check_100 == True)][['dbase','sector2000_50']].groupby('sector2000_50').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tabella con diametri min-max-ave e quantili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g50[['sector2000_50',\n",
    "        's_dsub_min','s_dsub_mean','s_dsub_q99','s_dsub_max','s_dsub_count',\n",
    "        's_dbase_min','s_dbase_mean','s_dbase_q99','s_dbase_max','s_dbase_count',\n",
    "        's_dsub_min_100','s_dsub_mean_100','s_dsub_q99_100','s_dsub_max_100','s_dsub_count_100',\n",
    "        's_dbase_min_100','s_dbase_mean_100','s_dbase_q99_100','s_dbase_max_100', 's_dbase_count_100']\n",
    "      ][df_g50.sector2000_50<=39].to_csv('diametriminmaxave.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zo.to_csv('df_zo.csv', sep=';')\n",
    "df_zo_100.to_csv('df_zo_100.csv', sep=';')\n",
    "df.to_csv('df.csv', sep=';')\n",
    "df0.to_csv('df0.csv', sep=';')\n",
    "df.to_csv('df.csv', sep=';')\n",
    "df_g50.to_csv('df_g50.csv', sep=';')\n",
    "df0_100.to_csv('df0_100.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ====================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per farlo partire senza far girare la macro caricare i file di salvataggio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zo = pd.read_csv('df_zo.csv', sep=';', index_col=0)\n",
    "df_zo_100 = pd.read_csv('df_zo_100.csv', sep=';', index_col=0)\n",
    "df = pd.read_csv('df.csv', sep=';', index_col=0)\n",
    "df0 = pd.read_csv('df0.csv', sep=';', index_col=0)\n",
    "df0_100 = pd.read_csv('df0_100.csv', sep=';', index_col=0)\n",
    "df = pd.read_csv('df.csv', sep=';', index_col=0)\n",
    "df_g50 = pd.read_csv('df_g50.csv', sep=';', index_col=0)\n",
    "df0_100 = pd.read_csv('df0_100.csv', sep=';', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotto tabelle separate per percentile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tabella deterministico 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deterministico --> Dati esistenti (da splittare in subsidenze Pds>0.1-0.001 e inbuti Pds>0.1)\n",
    "\n",
    "collist = [\n",
    "       'epoch', 'ril_ksmall', 'ril_klarge', 'tri_ksmall', 'tri_klarge',\n",
    "       'via_ksmall', 'via_klarge', 'pas_ksmall', 'pas_klarge']\n",
    "\n",
    "collistred = [\n",
    "       'ril_ksmall', 'ril_klarge', 'tri_ksmall', 'tri_klarge',\n",
    "       'via_ksmall', 'via_klarge', 'pas_ksmall', 'pas_klarge']\n",
    "\n",
    "df_grouped = pd.DataFrame({})\n",
    "\n",
    "for hh in range (int(1000 / (div_settori * m_punti) * pkfinale)):\n",
    "    df_temp = df0_100[df0_100.sector2000_50==hh][collist].groupby('epoch').sum()\n",
    "    \n",
    "    if df_temp.shape[0]==0: df_temp= pd.DataFrame(np.zeros((1,len(collistred))), columns=collistred)\n",
    "        \n",
    "    df_temp = df_temp.replace(np.nan, 0).apply(pd.to_numeric)\n",
    "    df_temp['index'] = hh\n",
    "    df_temp['sector2000_50'] = hh\n",
    "    ###\n",
    "    df_temp['vol_dave'] = df0_100[df0_100.sector2000_50==hh][['epoch','vol_dave']].groupby('epoch').sum().astype('int32')\n",
    "    '''\n",
    "    attenzione nel df_zo stochastico \n",
    "              dbase : diametro al piano campagna e \n",
    "              dave: diametro al contatto calcari/copertura\n",
    "    nel df0 invece è il contrario ma mi adatto al df_zo\n",
    "    '''\n",
    "\n",
    "    df_temp['n_karst_2000d'] = df0[(df0.sector2000_50==hh)\n",
    "                                  ][['sector2000_50']].count()[0]\n",
    "    df_temp['n_karst_100d'] = df0_100[(df0_100.sector2000_50==hh)\n",
    "                                  ][['sector2000_50']].count()[0]\n",
    "    \n",
    "\n",
    "    df_temp['dbase'] = df0_100[df0_100.sector2000_50==hh][['epoch','dave']].groupby('epoch').mean().astype('int32')\n",
    "    df_temp['dave'] = df0_100[df0_100.sector2000_50==hh][['epoch','dbase']].groupby('epoch').mean().astype('int32')\n",
    "    \n",
    "    df_temp.set_index('index', inplace=True)\n",
    "    df_grouped = df_grouped.append(df_temp)\n",
    "\n",
    "\n",
    "df_grouped['viaducts'] = df_grouped.via_klarge + df_grouped.via_ksmall\n",
    "df_grouped['bridges'] = df_grouped.pas_klarge + df_grouped.pas_ksmall\n",
    "df_grouped = df_grouped[['sector2000_50','dbase','dave','vol_dave','ril_klarge', 'ril_ksmall', 'tri_klarge',\n",
    "       'tri_ksmall', 'viaducts', 'bridges',\"n_karst_2000d\",\"n_karst_100d\"]]\n",
    "\n",
    "df_grouped_determ = df_grouped.fillna(0)\n",
    "df_grouped_determ.to_csv('df_grouped_determ.csv', sep=';')\n",
    "del df_grouped\n",
    "df_grouped_determ.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Karst superficiali e profondi nel df0   --> per stimare riempimento cavità m3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vol_sup = df0[df0.check_100==True][['sector2000_50','volume']].groupby('sector2000_50').sum()\n",
    "df_vol_sup.reset_index(inplace=True)\n",
    "df_vol_sup.rename(columns={df_vol_sup.columns.tolist()[-1]:'df0_volk_superf_100'}, inplace=True)\n",
    "df_vol_sup.to_csv('df_vol_sup.csv', sep=\";\")\n",
    "display(df_vol_sup.head(3))\n",
    "df_vol_prof = df0[df0.check_100==True][['sector2000_50','vol_dbase']].groupby('sector2000_50').sum()\n",
    "df_vol_prof.reset_index(inplace=True)\n",
    "df_vol_prof.rename(columns={df_vol_prof.columns.tolist()[-1]:'df0_volk_prof_100'}, inplace=True)\n",
    "df_vol_prof.to_csv('df_vol_prof.csv', sep=\";\")\n",
    "display(df_vol_prof.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creo la funzione per creare le tabelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from df_grouped95 -> to -> df_grouped_95_t ovvero completato con altre informazioni\n",
    "# from df_g50 ['sector2000_50', 'Litho', 'Tipo_plus','h_rilevato', 'h_rilevato_true', 'h_trincea', 'h_viadotto', 'h_copertura',\n",
    "# 'h_copertura_ril', 'h_copertura_tri', 'h_copertura_via', 'LRilevato', 'LTrincea', 'LViaduct', 'LBridge', 'n_karst', 'sector_area']\n",
    "# n_karst_2000_t sono i # karst nella banda 2km sia da deterministico (analisi karst in superficie) che da stochastico q95\n",
    "# n_karst_100_t sono i # karst come sopra ma su banda 100m\n",
    "# ril-klarge ril-ksmall tri_klarge tri_ksmall viaducts bridges sono i numeri di karst nella banda 100m\n",
    "\n",
    "\n",
    "def createdfgrouped(df_zo_100, df_zo, df_grouped_determ, df_g50, df_vol_prof, df_vol_sup, _quantile):\n",
    "    \n",
    "    collist = [\n",
    "           'epoch', 'ril_ksmall', 'ril_klarge', 'tri_ksmall', 'tri_klarge',\n",
    "           'via_ksmall', 'via_klarge', 'pas_ksmall', 'pas_klarge']\n",
    "    \n",
    "    list_col_g50 = ['sector2000_50','Litho','Tipo_plus','h_rilevato', 'h_rilevato_true','h_trincea','h_viadotto','h_copertura','h_copertura_ril','h_copertura_tri','h_copertura_via',\n",
    "                'LRilevato', 'LTrincea', 'LViaduct', 'LBridge','n_karst', 'sector_area']\n",
    "\n",
    "    df_grouped = pd.DataFrame({})\n",
    "\n",
    "    for hh in range (int(1000 / (div_settori * m_punti) * pkfinale)):\n",
    "        df_temp = df_zo_100[(df_zo_100.sector2000_50==hh)][collist].groupby('epoch').sum().quantile(_quantile).round(0)\n",
    "        df_temp['index'] = hh\n",
    "        df_temp['sector2000_50'] = hh\n",
    "\n",
    "        df_temp['vol_dave'] = df_zo_100[(df_zo_100['check_100']==True) & (df_zo_100.sector2000_50==hh)\n",
    "                                       ][['epoch','vol_dave']].groupby('epoch').sum().mean()[0].round(2)\n",
    "        \n",
    "        df_temp['h_copertura'] = df_zo_100[df_zo_100.sector2000_50==hh][['epoch','h_copertura']\n",
    "                                                                       ].groupby('epoch').mean().mean()[0].round(2)\n",
    "        \n",
    "        df_temp['dbase'] = df_zo_100[df_zo_100.sector2000_50==hh][['epoch','dbase']].groupby('epoch').mean().mean()[0].round(2)\n",
    "        df_temp['dave'] = df_zo_100[df_zo_100.sector2000_50==hh][['epoch','dave']].groupby('epoch').mean().mean()[0].round(2)\n",
    "        # df_temp = pd.to_numeric(df_temp.replace(np.nan, 0), downcast='integer')\n",
    "        \n",
    "        \n",
    "        # tolta condizione df_zo.Pds >=0.01 per inserirla deve essere congruente anche nelle righe sopra\n",
    "        # ad esempio in karst_via kars_ril ect\n",
    "        df_temp['n_karst_2000'] = df_zo[(df_zo.sector2000_50==hh)&(df_zo.dbase>0)\n",
    "                                          ][['sector2000_50','epoch']].groupby('epoch').count().quantile(_quantile)[0]\n",
    "        df_temp['n_karst_100'] = df_zo_100[(df_zo.sector2000_50==hh)&(df_zo_100.dbase>0)\n",
    "                                  ][['sector2000_50','epoch']].groupby('epoch').count().quantile(_quantile)[0]\n",
    "        df_grouped = df_grouped.append(df_temp)\n",
    "\n",
    "    df_grouped.set_index('index', inplace=True)\n",
    "    df_grouped['viaducts'] = df_grouped.via_klarge + df_grouped.via_ksmall\n",
    "    df_grouped['bridges'] = df_grouped.pas_klarge + df_grouped.pas_ksmall\n",
    "    df_grouped = df_grouped[['sector2000_50','dave','dbase','vol_dave','h_copertura','ril_klarge', 'ril_ksmall', 'tri_klarge',\n",
    "           'tri_ksmall', 'viaducts', 'bridges','n_karst_2000','n_karst_100']]\n",
    "    \n",
    "\n",
    "    ###\n",
    "    \n",
    "    cols_totsum = ['vol_dave','ril_klarge', 'ril_ksmall', 'tri_klarge', 'tri_ksmall', 'viaducts', 'bridges']\n",
    "\n",
    "    # somma algebrica dei due dataframe\n",
    "    df_temp = df_grouped[cols_totsum] + df_grouped_determ[cols_totsum]\n",
    "    # correggo le colonne che non devono essere sommate\n",
    "    df_temp[['dbase','dave','sector2000_50','n_karst_2000','n_karst_100']] = df_grouped[[\n",
    "        'dbase','dave','sector2000_50','n_karst_2000','n_karst_100']]\n",
    "    df_temp[['n_karst_2000d','n_karst_100d']] = df_grouped_determ[['n_karst_2000d','n_karst_100d']]\n",
    "\n",
    "    df_temp = pd.merge(df_temp, df_g50[list_col_g50], on='sector2000_50', left_index=True)\n",
    "    \n",
    "    # da eliminare\n",
    "    # n_karst arriva da df_g50\n",
    "    # df_temp = pd.merge(df_temp, df_grouped_2000[['sector2000_50','n_karst_2000_95']], on='sector2000_50', left_index=True)\n",
    "    # df_temp['n_karst_2000_95_t'] = df_temp['n_karst'] + df_temp['n_karst_2000_95']\n",
    "    \n",
    "    # sostituire t sta per totale\n",
    "    df_temp['n_karst_2000_t'] = df_temp['n_karst_2000d'] + df_temp['n_karst_2000']\n",
    "    df_temp['n_karst_100_t'] = df_temp['n_karst_100d'] + df_temp['n_karst_100']\n",
    "    \n",
    "    #\n",
    "    #df_temp = df_temp.drop(columns=['n_karst','n_karst_2000_95'])\n",
    "    df_temp['nk_kmq_100y'] = df_temp.n_karst_2000_t / df_temp.sector_area / vitautile\n",
    "    df_temp['nk_kmq_100y'] = df_temp['nk_kmq_100y'].round(3)\n",
    "    \n",
    "    # no perché al centro dell'offset hai molto meno errore\n",
    "    #df_temp['nk_kmq_100y_100'] = df_temp.n_karst_100_t / (df_temp.sector_area/2.5*0.1) / vitautile\n",
    "    df_temp['nk_kmq_100y_100'] = df_temp.n_karst_100_t / 0.2 / vitautile\n",
    "    df_temp['nk_kmq_100y_100'] = df_temp['nk_kmq_100y_100'].round(3)\n",
    "    \n",
    "\n",
    "    df_temp['LViabri'] = df_temp.LBridge + df_temp.LViaduct\n",
    "    df_temp['nk_viabri'] = df_temp.viaducts + df_temp.bridges\n",
    "    df_temp['nk_riltri'] = df_temp[['ril_klarge', 'ril_ksmall', 'tri_klarge', 'tri_ksmall']].sum(axis = 1)\n",
    "    \n",
    "    # nk_tot_100 è più alto a causa di approssimazioni per eccesso nel numero di carst per tipologia\n",
    "    df_temp['nk_tot_100'] = df_temp[['nk_viabri','nk_riltri']].sum(axis = 1)\n",
    "    df_temp['vol_dave_check'] = 4/3*np.pi*(df_temp.dave/2)**3 / 2 * df_temp['nk_tot_100']\n",
    "    df_temp = df_temp.merge(df_vol_sup, on='sector2000_50', how='left')\n",
    "    df_temp = df_temp.merge(df_vol_prof, on='sector2000_50', how='left')\n",
    "    \n",
    "    \n",
    "    # arrotondamenti\n",
    "    df_temp['h_rilevato'] = df_temp['h_rilevato'].round(2)\n",
    "    df_temp['sector_area'] = df_temp['sector_area'].round(2)\n",
    "    df_temp['h_copertura'] = df_temp['h_copertura'].round(2)\n",
    "    df_temp['h_copertura_ril'] = df_temp['h_copertura_ril'].round(2)\n",
    "    df_temp['h_copertura_tri'] = df_temp['h_copertura_tri'].round(2)\n",
    "    df_temp['h_copertura_via'] = df_temp['h_copertura_via'].round(2)\n",
    "    df_temp['h_trincea'] = df_temp['h_trincea'].round(2)\n",
    "    df_temp['h_viadotto'] = df_temp['h_viadotto'].round(2)\n",
    "    \n",
    "    return df_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creo le tabelle applico la funzione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### applica funzione\n",
    "df_grouped_95_t = createdfgrouped(df_zo_100, df_zo, df_grouped_determ, df_g50, df_vol_prof, df_vol_sup, 0.95)\n",
    "\n",
    "\n",
    "### Viadotti ###\n",
    "\n",
    "df_via_grouped_95_t = df_grouped_95_t[['Tipo_plus','sector2000_50','LViabri','nk_viabri',\n",
    "                                       'h_viadotto','h_copertura_via','nk_kmq_100y']]\n",
    "df_via_grouped_95_t = df_via_grouped_95_t.groupby('Tipo_plus').agg({\n",
    "    'sector2000_50':'first','LViabri':'sum','nk_viabri':'sum','h_viadotto':'max','h_copertura_via':'mean','nk_kmq_100y':'mean'\n",
    "    })\n",
    "\n",
    "df_via_grouped_95_t = df_via_grouped_95_t[(df_via_grouped_95_t.index != 'Rilevato') & (df_via_grouped_95_t.index != 'Trincea')]\n",
    "df_via_grouped_95_t['Tipo_viabri'] = np.where(df_via_grouped_95_t.h_viadotto >= lim_via_bridge, 'viaduct', 'bridge')\n",
    "\n",
    "### ultimi aggiustamenti ####\n",
    "\n",
    "df_grouped_95_t = df_grouped_95_t.fillna(0)\n",
    "df_via_grouped_95_t = df_via_grouped_95_t.fillna(0)\n",
    "\n",
    "df_grouped_95_t.head(4)\n",
    "display(df_grouped_95_t.head(4))\n",
    "display(df_via_grouped_95_t.head(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quantile 75   --> df_grouped_75_t & df_via_grouped_75_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### applica funzione\n",
    "df_grouped_75_t = createdfgrouped(df_zo_100, df_zo, df_grouped_determ, df_g50, df_vol_prof, df_vol_sup, 0.75)\n",
    "\n",
    "\n",
    "### Viadotti ###\n",
    "\n",
    "df_via_grouped_75_t = df_grouped_75_t[['Tipo_plus','sector2000_50','LViabri','nk_viabri',\n",
    "                                       'h_viadotto','h_copertura_via','nk_kmq_100y']]\n",
    "df_via_grouped_75_t = df_via_grouped_75_t.groupby('Tipo_plus').agg({\n",
    "    'sector2000_50':'first','LViabri':'sum','nk_viabri':'sum','h_viadotto':'max','h_copertura_via':'mean','nk_kmq_100y':'mean'\n",
    "    })\n",
    "\n",
    "df_via_grouped_75_t = df_via_grouped_75_t[(df_via_grouped_75_t.index != 'Rilevato') & (df_via_grouped_75_t.index != 'Trincea')]\n",
    "df_via_grouped_75_t['Tipo_viabri'] = np.where(df_via_grouped_75_t.h_viadotto >= lim_via_bridge, 'viaduct', 'bridge')\n",
    "\n",
    "### ultimi aggiustamenti ####\n",
    "\n",
    "df_grouped_75_t = df_grouped_75_t.fillna(0)\n",
    "df_via_grouped_75_t = df_via_grouped_75_t.fillna(0)\n",
    "\n",
    "df_grouped_75_t.head(4)\n",
    "display(df_grouped_75_t.head(4))\n",
    "display(df_via_grouped_75_t.head(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quantile 50   --> df_grouped_50_t & df_via_grouped_50_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### applica funzione\n",
    "df_grouped_50_t = createdfgrouped(df_zo_100, df_zo, df_grouped_determ, df_g50, df_vol_prof, df_vol_sup, 0.50)\n",
    "\n",
    "\n",
    "### Viadotti ###\n",
    "\n",
    "df_via_grouped_50_t = df_grouped_50_t[['Tipo_plus','sector2000_50','LViabri','nk_viabri',\n",
    "                                       'h_viadotto','h_copertura_via','nk_kmq_100y']]\n",
    "df_via_grouped_50_t = df_via_grouped_50_t.groupby('Tipo_plus').agg({\n",
    "    'sector2000_50':'first','LViabri':'sum','nk_viabri':'sum','h_viadotto':'max','h_copertura_via':'mean','nk_kmq_100y':'mean'\n",
    "    })\n",
    "\n",
    "df_via_grouped_50_t = df_via_grouped_50_t[(df_via_grouped_50_t.index != 'Rilevato') & (df_via_grouped_50_t.index != 'Trincea')]\n",
    "df_via_grouped_50_t['Tipo_viabri'] = np.where(df_via_grouped_50_t.h_viadotto >= lim_via_bridge, 'viaduct', 'bridge')\n",
    "\n",
    "### ultimi aggiustamenti ####\n",
    "\n",
    "df_grouped_50_t = df_grouped_50_t.fillna(0)\n",
    "df_via_grouped_50_t = df_via_grouped_50_t.fillna(0)\n",
    "\n",
    "df_grouped_50_t.head(4)\n",
    "display(df_grouped_50_t.head(4))\n",
    "display(df_via_grouped_50_t.head(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantile 25 --> df_grouped_25_t & df_via_grouped_25_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### applica funzione\n",
    "df_grouped_25_t = createdfgrouped(df_zo_100, df_zo, df_grouped_determ, df_g50, df_vol_prof, df_vol_sup, 0.25)\n",
    "\n",
    "\n",
    "### Viadotti ###\n",
    "\n",
    "df_via_grouped_25_t = df_grouped_25_t[['Tipo_plus','sector2000_50','LViabri','nk_viabri',\n",
    "                                       'h_viadotto','h_copertura_via','nk_kmq_100y']]\n",
    "df_via_grouped_25_t = df_via_grouped_25_t.groupby('Tipo_plus').agg({\n",
    "    'sector2000_50':'first','LViabri':'sum','nk_viabri':'sum','h_viadotto':'max','h_copertura_via':'mean','nk_kmq_100y':'mean'\n",
    "    })\n",
    "\n",
    "df_via_grouped_25_t = df_via_grouped_25_t[(df_via_grouped_25_t.index != 'Rilevato') & (df_via_grouped_25_t.index != 'Trincea')]\n",
    "df_via_grouped_25_t['Tipo_viabri'] = np.where(df_via_grouped_25_t.h_viadotto >= lim_via_bridge, 'viaduct', 'bridge')\n",
    "\n",
    "### ultimi aggiustamenti ####\n",
    "\n",
    "df_grouped_25_t = df_grouped_25_t.fillna(0)\n",
    "df_via_grouped_25_t = df_via_grouped_25_t.fillna(0)\n",
    "\n",
    "df_grouped_25_t.head(4)\n",
    "display(df_grouped_25_t.head(4))\n",
    "display(df_via_grouped_25_t.head(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CALCOLO GLI INTERVENTI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creo le funzioni per calcolare gli interventi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####       sostituisci h_rilevato con h_copertura_ril h_copertura_tri\n",
    "\n",
    "# trattamento rilevato_tipo per numero di karst per tratta\n",
    "# ridotto il 17/3 prima era 200\n",
    "tratt_pernk = 100\n",
    "\n",
    "def ril_tipo12b(row):\n",
    "    \n",
    "    if ((row['h_copertura_ril']<10) & (row['nk_kmq_100y']>=0.01)):\n",
    "        # rilevati\n",
    "        ltot1 = (int(row['ril_klarge']) + int(row['ril_ksmall'])) * tratt_pernk\n",
    "        if ltot1 > row['LRilevato'] : ltot1 = row['LRilevato']\n",
    "    else:\n",
    "        ltot1 = 0\n",
    "    \n",
    "    if ((row['h_copertura_tri']<10) & (row['nk_kmq_100y']>=0.01)):\n",
    "        # trincee\n",
    "        ltot2 = (int(row['tri_klarge']) + int(row['tri_ksmall'])) * tratt_pernk\n",
    "        if ltot2 > row['LTrincea'] : ltot2 = row['LTrincea']\n",
    "    else:\n",
    "        ltot2=0\n",
    "    \n",
    "    ltot = ltot1+ltot2\n",
    "    \n",
    "    return ltot\n",
    "\n",
    "def ril_tipo2(row):\n",
    "    # rilevati\n",
    "    if ((row['h_copertura_ril']<10)&(row['nk_kmq_100y']>=0.01)&(row['nk_kmq_100y']<0.1)) or \\\n",
    "    ((row.h_copertura_ril >= 10)&(row.h_copertura_ril < 15)&(row['nk_kmq_100y']>=0.01)) :\n",
    "        # rilevati\n",
    "        ltot1 = int(row['ril_klarge']) * tratt_pernk + int(row['ril_ksmall']) * tratt_pernk\n",
    "        if ltot1 > row['LRilevato'] : ltot1 = row['LRilevato']\n",
    "    else:\n",
    "        ltot1=0\n",
    "    # trincee\n",
    "    if ((row['h_copertura_tri']<10)&(row['nk_kmq_100y']>=0.01)&(row['nk_kmq_100y']<0.1)) or \\\n",
    "    ((row.h_copertura_tri >= 10)&(row.h_copertura_tri < 15)&(row['nk_kmq_100y']>=0.01)) :\n",
    "        # trincee\n",
    "        ltot2 = int(row['tri_klarge']) * tratt_pernk + int(row['tri_ksmall']) * tratt_pernk\n",
    "        if ltot2 > row['LTrincea'] : ltot2 = row['LTrincea']\n",
    "    else:\n",
    "        ltot2=0\n",
    "    \n",
    "    ltot = ltot1+ltot2\n",
    "    return ltot\n",
    "\n",
    "def ril_tipo3(row):\n",
    "    \n",
    "    if ((row['h_copertura_ril'] >= 15) & (row['h_copertura_ril'] < 30) & (row['nk_kmq_100y']>=0.01)):\n",
    "        # rilevati\n",
    "        ltot1 = int(row['ril_klarge']) * tratt_pernk + int(row['ril_ksmall']) * tratt_pernk\n",
    "        if ltot1 > row['LRilevato'] : ltot1 = row['LRilevato']\n",
    "    else:\n",
    "        ltot1 = 0\n",
    "        \n",
    "    if ((row['h_copertura_tri'] >= 15) & (row['h_copertura_tri'] < 30) & (row['nk_kmq_100y']>=0.01)):\n",
    "        # trincee\n",
    "        ltot2 = int(row['tri_klarge']) * tratt_pernk + int(row['tri_ksmall']) * tratt_pernk\n",
    "        if ltot2 > row['LTrincea'] : ltot2 = row['LTrincea'] \n",
    "    else:\n",
    "        ltot2=0\n",
    "        \n",
    "    ltot = ltot1+ltot2\n",
    "    return ltot\n",
    "\n",
    "def ril_tipo4(row):\n",
    "    \n",
    "    if ((row['h_copertura_ril'] >= 30) & (row['nk_kmq_100y']>=0.01)):\n",
    "        # rilevati\n",
    "        ltot1 = int(row['ril_klarge']) * tratt_pernk + int(row['ril_ksmall']) *tratt_pernk\n",
    "        if ltot1 > row['LRilevato'] : ltot1 = row['LRilevato']\n",
    "    else:\n",
    "        ltot1=0\n",
    "    \n",
    "    if ((row['h_copertura_tri'] >= 30) & (row['nk_kmq_100y']>=0.01)):\n",
    "        # trincee\n",
    "        ltot2 = int(row['tri_klarge']) * tratt_pernk + int(row['tri_ksmall']) * tratt_pernk\n",
    "        if ltot2 > row['LTrincea'] : ltot2 = row['LTrincea']\n",
    "    else:\n",
    "        ltot2=0\n",
    "    \n",
    "    ltot = ltot1+ltot2\n",
    "    return ltot\n",
    "\n",
    "\n",
    "def via_tipo1(row):\n",
    "    \n",
    "    if row['h_copertura_via']<6:\n",
    "        ltot = 1       \n",
    "    elif (row['Tipo_viabri']=='viaduct')&(row['nk_kmq_100y']<0.01)&(row['h_copertura_via']<10)&(row['h_copertura_via']>=6) :\n",
    "        ltot=1\n",
    "    else:\n",
    "        ltot = 0\n",
    "        \n",
    "    return ltot\n",
    "\n",
    "def via_tipo24(row):\n",
    "    \n",
    "    if (row['Tipo_viabri']=='bridge') & (row['h_copertura_via'] >= 6) & (row['h_copertura_via'] < 10) & (row['nk_kmq_100y']>=0.01):\n",
    "        ltot = 1           \n",
    "    else:\n",
    "        ltot = 0\n",
    "        \n",
    "    return ltot\n",
    "\n",
    "def via_tipo34(row):\n",
    "    \n",
    "    if (row['Tipo_viabri']=='viaduct') & (row['h_copertura_via'] >= 6) & (row['h_copertura_via'] <20) & (row['nk_kmq_100y']>=0.01):\n",
    "        ltot = 1           \n",
    "    else:\n",
    "        ltot = 0\n",
    "        \n",
    "    return ltot\n",
    "        \n",
    "def via_tipo5(row):\n",
    "    \n",
    "    if (row['Tipo_viabri']=='viaduct') & (row['h_copertura_via'] >= 10) & (row['h_copertura_via'] < 20) & (row['nk_kmq_100y'] < 0.01):\n",
    "        ltot = 1\n",
    "   \n",
    "    elif (row['Tipo_viabri']=='viaduct') & (row['h_copertura_via'] >= 20):\n",
    "        ltot = 1\n",
    "        \n",
    "    elif (row['Tipo_viabri']=='bridge') & (row['h_copertura_via'] >= 6) & (row['h_copertura_via'] < 10) & (row['nk_kmq_100y'] < 0.01):\n",
    "        ltot = 1\n",
    "        \n",
    "    elif (row['Tipo_viabri']=='bridge') & (row['h_copertura_via'] >= 10):\n",
    "        ltot = 1\n",
    "        \n",
    "    else:\n",
    "        ltot = 0        \n",
    "    return ltot\n",
    "\n",
    "# def ril_geogriglia(row):\n",
    "    \n",
    "#     # aggiungere in assenza di interventi di consolidamento*****************\n",
    "    \n",
    "#     if (row['h_copertura'] >= 15) & (row['h_copertura'] < 30) & (row['nk_kmq_100y'] > 0.01):pass\n",
    "    \n",
    "def ril_compactg(row):\n",
    "    # da aggiornare con proporzione tra subsidenze e inghiottitoi\n",
    "    #if (row['h_copertura']<30) & (row['h_copertura']>=10) & (row['nk_kmq_100y'] > 0.1) & ((row['ril_klarge']+row['tri_klarge'])>0):\n",
    "    #il vincolo sulle klarge non è da considerare\n",
    "    if row['nk_kmq_100y'] > 0.1:\n",
    "        try:\n",
    "            prop = (row['ril_klarge']+row['tri_klarge'])/((row['ril_klarge']+row['tri_klarge']+row['ril_ksmall']+row['tri_ksmall']))\n",
    "            #ltot = (row['ril_tipo2'] + row['ril_tipo3']) * prop\n",
    "            #17/03 impostato solo sul rilevato tipo2\n",
    "            ltot = (row['ril_tipo2']) * prop\n",
    "        except:\n",
    "            ltot=0\n",
    "    else:\n",
    "        ltot = 0\n",
    "        \n",
    "    return round(ltot, 1)\n",
    "\n",
    "def ril_gratic(row):\n",
    "    \n",
    "    try:\n",
    "        # proporzione di subsidenze con inghiottitoi (da aggiornare con quantità in funziuone del Pds)\n",
    "        prop = (row['ril_klarge']+row['tri_klarge'])/((row['ril_klarge']+row['tri_klarge']+row['ril_ksmall']+row['tri_ksmall']))\n",
    "        ltot = row['ril_tipo12b'] * (1-prop)\n",
    "    except:\n",
    "        ltot = 0\n",
    "\n",
    "    return round(ltot, 1)\n",
    "\n",
    "\n",
    "def ril_vibrof(row):\n",
    "    \n",
    "    if row['Litho'] == 's':\n",
    "        try:\n",
    "            # proporzione di subsidenze con inghiottitoi (da aggiornare con quantità in funziuone del Pds)\n",
    "            prop = (row['ril_klarge']+row['tri_klarge'])/((row['ril_klarge']+row['tri_klarge']+row['ril_ksmall']+row['tri_ksmall']))\n",
    "            ltot = row['ril_tipo12b'] * prop\n",
    "        except:\n",
    "            ltot = 0\n",
    "    else:\n",
    "        ltot = 0\n",
    "    \n",
    "    return round(ltot, 1)\n",
    "\n",
    "\n",
    "def ril_dyncomp(row):\n",
    "    \n",
    "#     if ((row['h_copertura']<10)&(row['nk_kmq_100y']>=0.01)&(row['nk_kmq_100y']<0.1) & (row['Litho']=='s')):\n",
    "#         ltot = row['ril_tipo2']\n",
    "\n",
    "    if row['Litho'] == 'a':\n",
    "        try:\n",
    "            # proporzione di subsidenze con inghiottitoi (da aggiornare con quantità in funziuone del Pds)\n",
    "            prop = (row['ril_klarge']+row['tri_klarge'])/((row['ril_klarge']+row['tri_klarge']+row['ril_ksmall']+row['tri_ksmall']))\n",
    "            ltot = row['ril_tipo12b'] * prop\n",
    "        except:\n",
    "            ltot = 0\n",
    "    else:\n",
    "        ltot = 0\n",
    "    \n",
    "    return round(ltot, 1)\n",
    "\n",
    "\n",
    "def geogri(row):\n",
    "    \n",
    "    # applico nei rilevati e trincee tipo 4 dove la copertura è inferiore a 50m\n",
    "    # aggiunto limite di applicazione lambda > 0.05 e riduzione di 50% ovunque 16/03/2021\n",
    "    # rilevati\n",
    "    if ((row.ril_tipo4) > 0) & (row.h_copertura_ril < 50) & (row.nk_kmq_100y > 0.05):\n",
    "        ltot1 = row.ril_tipo4 * row.LRilevato/(row.LRilevato + row.LTrincea) * 0.5\n",
    "    else:\n",
    "        ltot1 = 0\n",
    "    \n",
    "    # trincee\n",
    "    if ((row.ril_tipo4) > 0) & (row.h_copertura_tri < 50) & (row.nk_kmq_100y > 0.05):\n",
    "        ltot2 = row.ril_tipo4 * row.LTrincea/(row.LRilevato + row.LTrincea) * 0.5\n",
    "    else:\n",
    "        ltot2 = 0\n",
    "    \n",
    "    # applico nelle zone ril tipo 1-2-3 dove non ci sono interneventi\n",
    "    # no compctgr no vibrof no dyncomp no graticcio\n",
    "    ltota = row['ril_tipo12b'] + row['ril_tipo2'] + row['ril_tipo3']\n",
    "    ltotb = row['ril_compactg'] + row['ril_vibrof'] + row['ril_dyncomp'] + row['ril_gratic']\n",
    "    ltot3 = ltota - ltotb\n",
    "    if ltot3 < 0 : ltot3 = 0\n",
    "    \n",
    "    ltot = round(ltot1+ltot2+ltot3,1)\n",
    "    \n",
    "    return ltot\n",
    "\n",
    "def kgrout_vol_sup(row):\n",
    "    \n",
    "    if row.ril_gratic > 0:\n",
    "        vtot = row.df0_volk_superf_100\n",
    "    else:\n",
    "        vtot = 0\n",
    "        \n",
    "    return round(vtot,1)\n",
    "\n",
    "def kgrout_vol(row):\n",
    "    # in quanto nello stesso km posso avere più di un intervento (non sovrapposto ma consecutivo)\n",
    "    # devo calcolare la percentuale di area che ricade negli interventi che prevedono riempimenti\n",
    "    # aggiunta sconto 0.5 su tutte, ulteriore 0.5 se lambda<0.1 e 0 se lambda < 0.05 16/03/2021\n",
    "    try:\n",
    "        prop1 = (row.ril_gratic + row.geogri)/(row.ril_gratic + row.ril_compactg + row.ril_vibrof + row.ril_dyncomp + row.geogri)\n",
    "    except:\n",
    "        prop1 = 0\n",
    "        \n",
    "    prop2 = (30 - row.h_copertura)/30\n",
    "    if prop2 < 0 : prop2 = 0\n",
    "    \n",
    "    # 17/03\n",
    "    # cavità identificate iniziali da evidenze in superficie\n",
    "    v1 = row.df0_volk_prof_100\n",
    "    # cavità stimate senza evidenze in superficie (stimata sulla fascia 100m)\n",
    "    v2 = row.vol_dave - row.df0_volk_prof_100  \n",
    "    vtot = (v1 + v2 * 0.1) * prop1 * prop2 * 0.5\n",
    "    if row.nk_kmq_100y < 0.1 : vtot *= 0.5\n",
    "    if row.nk_kmq_100y < 0.05 : vtot = 0\n",
    "\n",
    "       \n",
    "    # condizione 16/03\n",
    "    #vtot = row.vol_dave * prop1 * prop2 * 0.5\n",
    "    #if row.nk_kmq_100y < 0.1 : vtot *= 0.5\n",
    "    #if row.nk_kmq_100y < 0.05 : vtot = 0.0\n",
    "    \n",
    "    return round(vtot,1)\n",
    "\n",
    "def l_dreni(row):\n",
    "    \n",
    "    passo_dreni = 20\n",
    "    lati = 2\n",
    "    \n",
    "    try:\n",
    "        h_copertura_media = (row.h_copertura_ril * row.LRilevato + row.h_copertura_tri * row.LTrincea)/(row.LRilevato + row.LTrincea)\n",
    "    except:\n",
    "        h_copertura_media = 0\n",
    "    \n",
    "    # lunghezza dreno sommata\n",
    "    #ltot = ((row.ril_tipo12b - row.ril_gratic) + row.ril_tipo2 + row.ril_tipo3)/passo_dreni*lati*h_copertura_media\n",
    "    # lunghezza di applicazione lungo il tracciato\n",
    "    #ltot = ((row.ril_tipo12b - row.ril_gratic) + row.ril_tipo2 + row.ril_tipo3)\n",
    "    # 17/03 riunione interna\n",
    "    #ltot = (row.ril_compactg + row.ril_vibrof + row.ril_dyncomp)\n",
    "    #17/10 riunione --> ril_vibrof e ril_dyncomp non si fanno al loro posto RC slab\n",
    "    ltot = (row.ril_compactg)\n",
    "    \n",
    "    \n",
    "    #  condizione aggiunta meeting 16/03/2021\n",
    "    # non piu valida\n",
    "    #if row.nk_kmq_100y < 0.1 : ltot *= 0.5\n",
    "    #if row.nk_kmq_100y < 0.05 : ltot = 0\n",
    "    \n",
    "    return ltot\n",
    "\n",
    "\n",
    "def l_trincea(row):\n",
    "    \n",
    "    # trincee sempre entrambe i lati quanto copertura < 20m\n",
    "    # nota se metto row.ril_tipo12b vuol dire che automaticamente escludo coperture >10m!\n",
    "    if (row.h_copertura_tri < 20) : \n",
    "        ltot1 = row.ril_tipo12b * row.LTrincea/(row.LRilevato + row.LTrincea) * 2\n",
    "    else:\n",
    "        ltot1 = 0\n",
    "        \n",
    "    # rilevati sempre un lato in base alla pendenza quando h<10\n",
    "    # un lato e quando h 10-20m per le zone ad altissimo rischio lambda > 0.3\n",
    "    # nota se metto row.ril_tipo12b vuol dire che automaticamente escludo coperture >10m!\n",
    "    if (row.h_copertura_ril < 10): \n",
    "        ltot2 = row.ril_tipo12b * row.LRilevato/(row.LRilevato + row.LTrincea) * 1\n",
    "    elif ((row.h_copertura_ril >= 10) & (row.h_copertura_ril < 20) & (row.nk_kmq_100y > 0.3)):\n",
    "        ltot2 = row.ril_tipo12b * row.LRilevato/(row.LRilevato + row.LTrincea) * 1\n",
    "    else:\n",
    "        ltot2 = 0  \n",
    "        \n",
    "    ltot = ltot1 + ltot2\n",
    "    \n",
    "    return ltot        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applico le funzioni PER 95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped_95_t['ril_tipo12b'] = df_grouped_95_t.apply(ril_tipo12b, axis=1)\n",
    "df_grouped_95_t['ril_tipo2'] = df_grouped_95_t.apply(ril_tipo2, axis=1)\n",
    "df_grouped_95_t['ril_tipo3'] = df_grouped_95_t.apply(ril_tipo3, axis=1)\n",
    "df_grouped_95_t['ril_tipo4'] = df_grouped_95_t.apply(ril_tipo4, axis=1)\n",
    "\n",
    "df_grouped_95_t = df_grouped_95_t.fillna(0)\n",
    "\n",
    "\n",
    "df_via_grouped_95_t['via_tipo1'] = df_via_grouped_95_t.apply(via_tipo1, axis=1)\n",
    "df_via_grouped_95_t['via_tipo24'] = df_via_grouped_95_t.apply(via_tipo24, axis=1)\n",
    "df_via_grouped_95_t['via_tipo34'] = df_via_grouped_95_t.apply(via_tipo34, axis=1)\n",
    "df_via_grouped_95_t['via_tipo5'] = df_via_grouped_95_t.apply(via_tipo5, axis=1)\n",
    "\n",
    "df_via_grouped_95_t = df_via_grouped_95_t.fillna(0)\n",
    "\n",
    "df_grouped_95_t['ril_compactg'] = df_grouped_95_t.apply(ril_compactg, axis=1)\n",
    "df_grouped_95_t['ril_vibrof'] = df_grouped_95_t.apply(ril_vibrof, axis=1)\n",
    "df_grouped_95_t['ril_dyncomp'] = df_grouped_95_t.apply(ril_dyncomp, axis=1)\n",
    "df_grouped_95_t['ril_gratic'] = df_grouped_95_t.apply(ril_gratic, axis=1)\n",
    "df_grouped_95_t['ril_gratic_vibr_dyn'] = df_grouped_95_t[['ril_gratic','ril_vibrof','ril_dyncomp']].sum(axis=1)\n",
    "df_grouped_95_t['geogri'] = df_grouped_95_t.apply(geogri, axis=1)\n",
    "df_grouped_95_t['kgrout_vol'] = df_grouped_95_t.apply(kgrout_vol, axis=1)\n",
    "df_grouped_95_t['kgrout_vol_sup'] = df_grouped_95_t.apply(kgrout_vol_sup, axis=1)\n",
    "df_grouped_95_t['l_dreni'] = round(df_grouped_95_t.apply(l_dreni, axis=1),0)\n",
    "df_grouped_95_t['l_trincea'] = round(df_grouped_95_t.apply(l_trincea, axis=1),0)\n",
    "\n",
    "df_grouped_95_t = df_grouped_95_t.fillna(0)\n",
    "\n",
    "display(df_grouped_95_t.head(4))\n",
    "\n",
    "df_temp = df_via_grouped_95_t[['sector2000_50','Tipo_viabri','h_copertura_via',\n",
    "       'h_viadotto', 'LViabri', 'nk_kmq_100y', 'via_tipo1', 'via_tipo24',\n",
    "       'via_tipo34', 'via_tipo5']]\n",
    "df_temp.to_csv('df_via_grouped_95_t.csv',sep=';')\n",
    "display(df_temp.head(10))\n",
    "\n",
    "df_temp = df_grouped_95_t[(df_grouped_95_t.ril_tipo12b>0) | (df_grouped_95_t.ril_tipo2>0) | (df_grouped_95_t.ril_tipo3>0) | (df_grouped_95_t.ril_tipo4>0)][['sector2000_50',\n",
    "       'h_copertura','h_copertura_ril', 'h_copertura_tri','Litho',\n",
    "       'LRilevato', 'LTrincea',\n",
    "       'nk_kmq_100y', 'ril_tipo12b',\n",
    "       'ril_tipo2', 'ril_tipo3', 'ril_tipo4', 'ril_gratic','ril_gratic_vibr_dyn','ril_compactg', 'ril_vibrof', 'ril_dyncomp','geogri',\n",
    "       'kgrout_vol_sup','kgrout_vol','l_dreni','l_trincea']].fillna(0)\n",
    "df_temp.to_csv('df_grouped_95_t.csv', sep=';')\n",
    "display(df_temp.head(41))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CALCOLO INTERVENTI q75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped_75_t['ril_tipo12b'] = df_grouped_75_t.apply(ril_tipo12b, axis=1)\n",
    "df_grouped_75_t['ril_tipo2'] = df_grouped_75_t.apply(ril_tipo2, axis=1)\n",
    "df_grouped_75_t['ril_tipo3'] = df_grouped_75_t.apply(ril_tipo3, axis=1)\n",
    "df_grouped_75_t['ril_tipo4'] = df_grouped_75_t.apply(ril_tipo4, axis=1)\n",
    "\n",
    "df_grouped_75_t = df_grouped_75_t.fillna(0)\n",
    "\n",
    "\n",
    "df_via_grouped_75_t['via_tipo1'] = df_via_grouped_75_t.apply(via_tipo1, axis=1)\n",
    "df_via_grouped_75_t['via_tipo24'] = df_via_grouped_75_t.apply(via_tipo24, axis=1)\n",
    "df_via_grouped_75_t['via_tipo34'] = df_via_grouped_75_t.apply(via_tipo34, axis=1)\n",
    "df_via_grouped_75_t['via_tipo5'] = df_via_grouped_75_t.apply(via_tipo5, axis=1)\n",
    "\n",
    "df_via_grouped_75_t = df_via_grouped_75_t.fillna(0)\n",
    "\n",
    "df_grouped_75_t['ril_compactg'] = df_grouped_75_t.apply(ril_compactg, axis=1)\n",
    "df_grouped_75_t['ril_vibrof'] = df_grouped_75_t.apply(ril_vibrof, axis=1)\n",
    "df_grouped_75_t['ril_dyncomp'] = df_grouped_75_t.apply(ril_dyncomp, axis=1)\n",
    "df_grouped_75_t['ril_gratic'] = df_grouped_75_t.apply(ril_gratic, axis=1)\n",
    "df_grouped_75_t['ril_gratic_vibr_dyn'] = df_grouped_75_t[['ril_gratic','ril_vibrof','ril_dyncomp']].sum(axis=1)\n",
    "df_grouped_75_t['geogri'] = df_grouped_75_t.apply(geogri, axis=1)\n",
    "df_grouped_75_t['kgrout_vol'] = df_grouped_75_t.apply(kgrout_vol, axis=1)\n",
    "df_grouped_75_t['kgrout_vol_sup'] = df_grouped_75_t.apply(kgrout_vol_sup, axis=1)\n",
    "df_grouped_75_t['l_dreni'] = round(df_grouped_75_t.apply(l_dreni, axis=1),0)\n",
    "df_grouped_75_t['l_trincea'] = round(df_grouped_75_t.apply(l_trincea, axis=1),0)\n",
    "\n",
    "df_grouped_75_t = df_grouped_75_t.fillna(0)\n",
    "\n",
    "display(df_grouped_75_t.head(4))\n",
    "\n",
    "df_temp = df_via_grouped_75_t[['sector2000_50','Tipo_viabri','h_copertura_via',\n",
    "       'h_viadotto', 'LViabri', 'nk_kmq_100y', 'via_tipo1', 'via_tipo24',\n",
    "       'via_tipo34', 'via_tipo5']]\n",
    "df_temp.to_csv('df_via_grouped_75_t.csv',sep=';')\n",
    "display(df_temp.head(4))\n",
    "\n",
    "df_temp = df_grouped_75_t[(df_grouped_75_t.ril_tipo12b>0) | (df_grouped_75_t.ril_tipo2>0) | (df_grouped_75_t.ril_tipo3>0) | (df_grouped_75_t.ril_tipo4>0)][['sector2000_50',\n",
    "       'h_copertura','h_copertura_ril', 'h_copertura_tri','Litho',\n",
    "       'LRilevato', 'LTrincea',\n",
    "       'nk_kmq_100y', 'ril_tipo12b',\n",
    "       'ril_tipo2', 'ril_tipo3', 'ril_tipo4', 'ril_gratic','ril_gratic_vibr_dyn','ril_compactg', 'ril_vibrof', 'ril_dyncomp','geogri',\n",
    "       'kgrout_vol_sup','kgrout_vol','l_dreni','l_trincea']].fillna(0)\n",
    "df_temp.to_csv('df_grouped_75_t.csv', sep=';')\n",
    "display(df_temp.head(41))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CALCOLO INTERVENTI q50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_grouped_50_t['ril_tipo12b'] = df_grouped_50_t.apply(ril_tipo12b, axis=1)\n",
    "df_grouped_50_t['ril_tipo2'] = df_grouped_50_t.apply(ril_tipo2, axis=1)\n",
    "df_grouped_50_t['ril_tipo3'] = df_grouped_50_t.apply(ril_tipo3, axis=1)\n",
    "df_grouped_50_t['ril_tipo4'] = df_grouped_50_t.apply(ril_tipo4, axis=1)\n",
    "\n",
    "df_grouped_50_t = df_grouped_50_t.fillna(0)\n",
    "\n",
    "\n",
    "df_via_grouped_50_t['via_tipo1'] = df_via_grouped_50_t.apply(via_tipo1, axis=1)\n",
    "df_via_grouped_50_t['via_tipo24'] = df_via_grouped_50_t.apply(via_tipo24, axis=1)\n",
    "df_via_grouped_50_t['via_tipo34'] = df_via_grouped_50_t.apply(via_tipo34, axis=1)\n",
    "df_via_grouped_50_t['via_tipo5'] = df_via_grouped_50_t.apply(via_tipo5, axis=1)\n",
    "\n",
    "df_via_grouped_50_t = df_via_grouped_50_t.fillna(0)\n",
    "\n",
    "df_grouped_50_t['ril_compactg'] = df_grouped_50_t.apply(ril_compactg, axis=1)\n",
    "df_grouped_50_t['ril_vibrof'] = df_grouped_50_t.apply(ril_vibrof, axis=1)\n",
    "df_grouped_50_t['ril_dyncomp'] = df_grouped_50_t.apply(ril_dyncomp, axis=1)\n",
    "df_grouped_50_t['ril_gratic'] = df_grouped_50_t.apply(ril_gratic, axis=1)\n",
    "df_grouped_50_t['ril_gratic_vibr_dyn'] = df_grouped_50_t[['ril_gratic','ril_vibrof','ril_dyncomp']].sum(axis=1)\n",
    "df_grouped_50_t['geogri'] = df_grouped_50_t.apply(geogri, axis=1)\n",
    "df_grouped_50_t['kgrout_vol'] = df_grouped_50_t.apply(kgrout_vol, axis=1)\n",
    "df_grouped_50_t['kgrout_vol_sup'] = df_grouped_50_t.apply(kgrout_vol_sup, axis=1)\n",
    "df_grouped_50_t['l_dreni'] = round(df_grouped_50_t.apply(l_dreni, axis=1),0)\n",
    "df_grouped_50_t['l_trincea'] = round(df_grouped_50_t.apply(l_trincea, axis=1),0)\n",
    "\n",
    "df_grouped_50_t = df_grouped_50_t.fillna(0)\n",
    "\n",
    "display(df_grouped_50_t.head(4))\n",
    "\n",
    "df_temp = df_via_grouped_50_t[['sector2000_50','Tipo_viabri','h_copertura_via',\n",
    "       'h_viadotto', 'LViabri', 'nk_kmq_100y', 'via_tipo1', 'via_tipo24',\n",
    "       'via_tipo34', 'via_tipo5']]\n",
    "df_temp.to_csv('df_via_grouped_50_t.csv',sep=';')\n",
    "display(df_temp.head(4))\n",
    "\n",
    "df_temp = df_grouped_50_t[(df_grouped_50_t.ril_tipo12b>0) | (df_grouped_50_t.ril_tipo2>0) | (df_grouped_50_t.ril_tipo3>0) | (df_grouped_50_t.ril_tipo4>0)][['sector2000_50',\n",
    "       'h_copertura', 'h_copertura_ril', 'h_copertura_tri','Litho',\n",
    "       'LRilevato', 'LTrincea',\n",
    "       'nk_kmq_100y', 'ril_tipo12b',\n",
    "       'ril_tipo2', 'ril_tipo3', 'ril_tipo4', 'ril_gratic','ril_gratic_vibr_dyn','ril_compactg', 'ril_vibrof', 'ril_dyncomp','geogri',\n",
    "       'kgrout_vol_sup','kgrout_vol','l_dreni','l_trincea']].fillna(0)\n",
    "df_temp.to_csv('df_grouped_50_t.csv', sep=';')\n",
    "display(df_temp.head(41))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CALCOLO INTERVENTI q25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped_25_t['ril_tipo12b'] = df_grouped_25_t.apply(ril_tipo12b, axis=1)\n",
    "df_grouped_25_t['ril_tipo2'] = df_grouped_25_t.apply(ril_tipo2, axis=1)\n",
    "df_grouped_25_t['ril_tipo3'] = df_grouped_25_t.apply(ril_tipo3, axis=1)\n",
    "df_grouped_25_t['ril_tipo4'] = df_grouped_25_t.apply(ril_tipo4, axis=1)\n",
    "\n",
    "df_grouped_25_t = df_grouped_25_t.fillna(0)\n",
    "\n",
    "\n",
    "df_via_grouped_25_t['via_tipo1'] = df_via_grouped_25_t.apply(via_tipo1, axis=1)\n",
    "df_via_grouped_25_t['via_tipo24'] = df_via_grouped_25_t.apply(via_tipo24, axis=1)\n",
    "df_via_grouped_25_t['via_tipo34'] = df_via_grouped_25_t.apply(via_tipo34, axis=1)\n",
    "df_via_grouped_25_t['via_tipo5'] = df_via_grouped_25_t.apply(via_tipo5, axis=1)\n",
    "\n",
    "df_via_grouped_25_t = df_via_grouped_25_t.fillna(0)\n",
    "\n",
    "df_grouped_25_t['ril_compactg'] = df_grouped_25_t.apply(ril_compactg, axis=1)\n",
    "df_grouped_25_t['ril_vibrof'] = df_grouped_25_t.apply(ril_vibrof, axis=1)\n",
    "df_grouped_25_t['ril_dyncomp'] = df_grouped_25_t.apply(ril_dyncomp, axis=1)\n",
    "df_grouped_25_t['ril_gratic'] = df_grouped_25_t.apply(ril_gratic, axis=1)\n",
    "df_grouped_25_t['ril_gratic_vibr_dyn'] = df_grouped_25_t[['ril_gratic','ril_vibrof','ril_dyncomp']].sum(axis=1)\n",
    "df_grouped_25_t['geogri'] = df_grouped_25_t.apply(geogri, axis=1)\n",
    "df_grouped_25_t['kgrout_vol'] = df_grouped_25_t.apply(kgrout_vol, axis=1)\n",
    "df_grouped_25_t['kgrout_vol_sup'] = df_grouped_25_t.apply(kgrout_vol_sup, axis=1)\n",
    "df_grouped_25_t['l_dreni'] = round(df_grouped_25_t.apply(l_dreni, axis=1),0)\n",
    "df_grouped_25_t['l_trincea'] = round(df_grouped_25_t.apply(l_trincea, axis=1),0)\n",
    "\n",
    "df_grouped_25_t = df_grouped_25_t.fillna(0)\n",
    "\n",
    "display(df_grouped_25_t.head(4))\n",
    "\n",
    "df_temp = df_via_grouped_25_t[['sector2000_50','Tipo_viabri','h_copertura_via',\n",
    "       'h_viadotto', 'LViabri', 'nk_kmq_100y', 'via_tipo1', 'via_tipo24',\n",
    "       'via_tipo34', 'via_tipo5']]\n",
    "df_temp.to_csv('df_via_grouped_25_t.csv',sep=';')\n",
    "display(df_temp.head(4))\n",
    "\n",
    "df_temp = df_grouped_25_t[(df_grouped_25_t.ril_tipo12b>0) | (df_grouped_25_t.ril_tipo2>0) | (df_grouped_25_t.ril_tipo3>0) | (df_grouped_25_t.ril_tipo4>0)][['sector2000_50',\n",
    "       'h_copertura', 'h_copertura_ril', 'h_copertura_tri','Litho',\n",
    "       'LRilevato', 'LTrincea',\n",
    "       'nk_kmq_100y', 'ril_tipo12b',\n",
    "       'ril_tipo2', 'ril_tipo3', 'ril_tipo4', 'ril_gratic','ril_gratic_vibr_dyn','ril_compactg', 'ril_vibrof', 'ril_dyncomp','geogri',\n",
    "       'kgrout_vol_sup','kgrout_vol','l_dreni','l_trincea']].fillna(0)\n",
    "# file per salvataggio con colonne selezionate\n",
    "df_temp.to_csv('df_grouped_25_t.csv', sep=';')\n",
    "display(df_temp.head(41))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### aggiorno df_g50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g50.merge(df_grouped_25_t[['sector2000_50','nk_tot']].rename(columns={'nk_tot':'nk_tot_p25'}), on = 'sector2000_50', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLOTTO DISTRIBUZIONI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(\n",
    "    data=df0[df0.type == 'Inbuto'], x=\"h_copertura\", y=\"dave\", hue=\"Litho\", fill=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "    data=df0.groupby('sector2000_50').agg({'h_copertura':'mean', 'long':'count', 'type':'max', 'Litho':'max', 'dave':'mean'}), \n",
    "    x=\"long\", y=\"h_copertura\", hue='Litho', sizes='dave'\n",
    ")\n",
    "plt.xlabel('n° di occorrenze')\n",
    "plt.xlim(0,160)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(\n",
    "    data=df0[df0.type == 'Subsidenza'], x=\"h_copertura\", y=\"dave\", hue=\"Litho\", fill=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "numero_righe = 8\n",
    "numero_colonne = 5\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(numero_righe, numero_colonne, sharex=True, figsize=(18,24))\n",
    "\n",
    "h=0\n",
    "\n",
    "for i in range(numero_righe):\n",
    "    for e in range(numero_colonne):\n",
    "\n",
    "        histdata1 = df_zo_100[(df_zo_100.sector2000_50 == h) & (df_zo_100['check_100']==True)].groupby('epoch').sum().ril_klarge\n",
    "        histdata2 = df_zo_100[(df_zo_100.sector2000_50 == h) & (df_zo_100['check_100']==True)].groupby('epoch').sum().tri_klarge\n",
    "        histdata = histdata1.append(histdata2)\n",
    "\n",
    "        sns.kdeplot(histdata, color='grey', fill=True, ax=axs[i, e])\n",
    "        axs[i, e].set_title('Sector: '+ str(h))\n",
    "\n",
    "        # Add labels\n",
    "        axs[i, e].set_xlabel('Number of karst')\n",
    "        axs[i, e].set_ylabel('Frequency')\n",
    "        axs[i, e].set_xlim(0,5)\n",
    "        axs[i, e].set_xticks([0,1,2,3,4,5])\n",
    "        axs[i, e].set_yticks([])\n",
    "        _ymin, _ymax = axs[i, e].get_ylim()\n",
    "        axs[i, e].vlines(np.quantile(histdata,0.25), _ymin, _ymax, colors = 'orange', linestyles= 'dashed',  linewidth=1)\n",
    "        axs[i, e].vlines(np.quantile(histdata,0.75), _ymin, _ymax, colors = 'orange', linestyles= 'dashed',  linewidth=1)\n",
    "        axs[i, e].vlines(np.quantile(histdata,0.95), _ymin, _ymax, colors = 'orange', linestyles= 'dashed',  linewidth=1)\n",
    "        axs[i, e].vlines(np.quantile(histdata,0.50), _ymin, _ymax, colors = 'red', linestyles= 'dashed',  linewidth=2)\n",
    "        axs[i, e].text(.5,.9,'centered title',\n",
    "                horizontalalignment='center',\n",
    "                transform=ax.transAxes)\n",
    "        h += 1\n",
    "\n",
    "plt.savefig('kde_klarge.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numero_righe = 8\n",
    "numero_colonne = 5\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(numero_righe, numero_colonne, sharex=True, figsize=(18,24))\n",
    "\n",
    "h=0\n",
    "\n",
    "for i in range(numero_righe):\n",
    "    for e in range(numero_colonne):\n",
    "\n",
    "        histdata1 = df_zo_100[(df_zo_100.sector2000_50 == h) & (df_zo_100['check_100']==True)].groupby('epoch').sum().ril_ksmall\n",
    "        histdata2 = df_zo_100[(df_zo_100.sector2000_50 == h) & (df_zo_100['check_100']==True)].groupby('epoch').sum().tri_ksmall\n",
    "        histdata = histdata1.append(histdata2)\n",
    "\n",
    "        sns.kdeplot(histdata, color='grey', fill=True, ax=axs[i, e])\n",
    "        axs[i, e].set_title('Sector: '+ str(h))\n",
    "\n",
    "        # Add labels\n",
    "        axs[i, e].set_xlabel('Number of karst')\n",
    "        axs[i, e].set_ylabel('Frequency')\n",
    "        axs[i, e].set_xlim(0,5)\n",
    "        axs[i, e].set_xticks([0,1,2,3,4,5])\n",
    "        axs[i, e].set_yticks([])\n",
    "        _ymin, _ymax = axs[i, e].get_ylim()\n",
    "        axs[i, e].vlines(np.quantile(histdata,0.25), _ymin, _ymax, colors = 'orange', linestyles= 'dashed',  linewidth=1)\n",
    "        axs[i, e].vlines(np.quantile(histdata,0.75), _ymin, _ymax, colors = 'orange', linestyles= 'dashed',  linewidth=1)\n",
    "        axs[i, e].vlines(np.quantile(histdata,0.95), _ymin, _ymax, colors = 'orange', linestyles= 'dashed',  linewidth=1)\n",
    "        axs[i, e].vlines(np.quantile(histdata,0.50), _ymin, _ymax, colors = 'red', linestyles= 'dashed',  linewidth=2)\n",
    "        h += 1\n",
    "\n",
    "plt.savefig('kde_ksmall.png')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Viadotti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "numero_righe = 4\n",
    "numero_colonne = 5\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(numero_righe, numero_colonne, sharex=True, figsize=(18,12))\n",
    "\n",
    "h = 0\n",
    "\n",
    "for i in range(numero_righe):\n",
    "    for e in range(numero_colonne):\n",
    "        try:\n",
    "            histdata1 = df_zo_100[(df_zo_100.sector2000_50 == h) & (df_zo_100['check_100']==True)].groupby('epoch').sum().via_klarge\n",
    "            histdata2 = df_zo_100[(df_zo_100.sector2000_50 == h) & (df_zo_100['check_100']==True)].groupby('epoch').sum().via_ksmall\n",
    "            histdata3 = df_zo_100[(df_zo_100.sector2000_50 == h) & (df_zo_100['check_100']==True)].groupby('epoch').sum().pas_klarge\n",
    "            histdata4 = df_zo_100[(df_zo_100.sector2000_50 == h) & (df_zo_100['check_100']==True)].groupby('epoch').sum().pas_ksmall\n",
    "            histdata = histdata1.append(histdata2).append(histdata3).append(histdata4)\n",
    "            \n",
    "            sns.histplot(histdata, element='step', stat='density', ax=axs[i, e], color='c')\n",
    "            sns.kdeplot(histdata, color='grey', fill=True, ax=axs[i, e])\n",
    "            axs[i, e].set_title('Sector: '+ str(h))\n",
    "\n",
    "            # Add labels\n",
    "            axs[i, e].set_xlabel('Number of karst')\n",
    "            axs[i, e].set_ylabel('Frequency')\n",
    "            axs[i, e].set_xlim(0,5)\n",
    "            _ymin, _ymax = axs[i, e].get_ylim()\n",
    "            axs[i, e].vlines(np.quantile(histdata,0.25), _ymin, _ymax, colors = 'lightgreen', linestyles= 'dashed',  linewidth=1)\n",
    "            axs[i, e].vlines(np.quantile(histdata,0.75), _ymin, _ymax, colors = 'orange', linestyles= 'dashed',  linewidth=1)\n",
    "            axs[i, e].vlines(np.quantile(histdata,0.95), _ymin, _ymax, colors = 'red', linestyles= 'dashed',  linewidth=1)\n",
    "            axs[i, e].vlines(np.quantile(histdata,0.50), _ymin, _ymax, colors = 'k', linestyles= 'dashed',  linewidth=1)\n",
    "            axs[i, e].text(.5,.9,'centered title',\n",
    "                horizontalalignment='center',\n",
    "                transform=axs[i, e].transAxes)\n",
    "        except:\n",
    "            pass\n",
    "        h += 1\n",
    "\n",
    "plt.savefig('kde_via_pas.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numero_righe = 8\n",
    "numero_colonne = 5\n",
    "\n",
    "\n",
    "# fig, axs = plt.subplots(numero_righe, numero_colonne, sharex=True, sharey=True, figsize=(18,12))\n",
    "fig, axs = plt.subplots(numero_righe, numero_colonne, figsize=(18,30))\n",
    "\n",
    "h = 0\n",
    "_xmax = 0.5\n",
    "_nbins = 5\n",
    "_ymax = 75\n",
    "\n",
    "for i in range(numero_righe):\n",
    "    for e in range(numero_colonne):\n",
    "        try:\n",
    "            histdata1 = df_zo[(df_zo.sector2000_50 == h)&(df_zo.dbase>0)].groupby('epoch').count().long\n",
    "            histdata_det = df_g50[(df_g50.sector2000_50==h)].n_karst\n",
    "            histdata =  (histdata1 + histdata_det.values[0])/ df_g50.sector_area.mean() /100\n",
    "            \n",
    "            sns.histplot(histdata, element='step', stat='density', ax=axs[i, e], color='c')\n",
    "            sns.kdeplot(histdata, color='grey', fill=True, ax=axs[i, e])\n",
    "            #axs[i, e].set_title('Sector: '+ str(h))\n",
    "\n",
    "            # Add labels\n",
    "            axs[i, e].set_xlabel('Number of karst')\n",
    "            axs[i, e].set_ylabel('')\n",
    "            #axs[i, e].set_xlim(0,_xmax)\n",
    "            #axs[i, e].set_ylim(0,_ymax)\n",
    "            _ymin, _ymax = axs[i, e].get_ylim()\n",
    "            axs[i, e].vlines(np.quantile(histdata,0.25), _ymin, _ymax, colors = 'lightgreen', linestyles= 'dashed',  linewidth=1)\n",
    "            axs[i, e].vlines(np.quantile(histdata,0.75), _ymin, _ymax, colors = 'orange', linestyles= 'dashed',  linewidth=1)\n",
    "            axs[i, e].vlines(np.quantile(histdata,0.95), _ymin, _ymax, colors = 'red', linestyles= 'dashed',  linewidth=1)\n",
    "            axs[i, e].vlines(np.quantile(histdata,0.50), _ymin, _ymax, colors = 'k', linestyles= 'dashed',  linewidth=1)\n",
    "            axs[i, e].text(.5,.9,'Sector: '+ str(h),\n",
    "                horizontalalignment='center',\n",
    "                transform=axs[i, e].transAxes)\n",
    "        except:\n",
    "            pass\n",
    "        h += 1\n",
    "plt.locator_params(axis=\"x\", nbins=_nbins)\n",
    "plt.savefig('kde_all_2000.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numero_righe = 8\n",
    "numero_colonne = 5\n",
    "\n",
    "\n",
    "# fig, axs = plt.subplots(numero_righe, numero_colonne, sharex=True, sharey=True, figsize=(18,12))\n",
    "fig, axs = plt.subplots(numero_righe, numero_colonne, figsize=(18,30))\n",
    "\n",
    "h = 0\n",
    "_xmax = 5\n",
    "_nbins = 5\n",
    "\n",
    "for i in range(numero_righe):\n",
    "    for e in range(numero_colonne):\n",
    "        try:\n",
    "            histdata1 = df_zo[(df_zo.sector2000_50 == h)&(df_zo.check_100 == True)&(df_zo.dbase>0)].groupby('epoch').count().long\n",
    "            histdata_det = df_g50[(df_g50.sector2000_50==h)].n_karst\n",
    "            histdata =  (histdata1 + histdata_det.values[0])\n",
    "            \n",
    "            sns.histplot(histdata, element='step', stat='density', ax=axs[i, e], color='c')\n",
    "            sns.kdeplot(histdata, color='grey', fill=True, ax=axs[i, e])\n",
    "            #axs[i, e].set_title('Sector: '+ str(h))\n",
    "\n",
    "            # Add labels\n",
    "            axs[i, e].set_xlabel('Number of karst')\n",
    "            axs[i, e].set_ylabel('')\n",
    "            #axs[i, e].set_xlim(0,_xmax)\n",
    "            #axs[i, e].set_ylim(0,_ymax)\n",
    "            _ymin, _ymax = axs[i, e].get_ylim()\n",
    "            axs[i, e].vlines(np.quantile(histdata,0.25), _ymin, _ymax, colors = 'lightgreen', linestyles= 'dashed',  linewidth=1)\n",
    "            axs[i, e].vlines(np.quantile(histdata,0.75), _ymin, _ymax, colors = 'orange', linestyles= 'dashed',  linewidth=1)\n",
    "            axs[i, e].vlines(np.quantile(histdata,0.95), _ymin, _ymax, colors = 'red', linestyles= 'dashed',  linewidth=1)\n",
    "            axs[i, e].vlines(np.quantile(histdata,0.50), _ymin, _ymax, colors = 'k', linestyles= 'dashed',  linewidth=1)\n",
    "            axs[i, e].text(.5,.9,'Sector: '+ str(h),\n",
    "                horizontalalignment='center',\n",
    "                transform=axs[i, e].transAxes)\n",
    "        except:\n",
    "            pass\n",
    "        h += 1\n",
    "plt.locator_params(axis=\"x\", nbins=_nbins)\n",
    "plt.savefig('kde_all_100.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLOTTO CON PLOTLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zo.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot  epoch singolo --> per diametri a piano campagna, Pds > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "\n",
    "# tracciato principale\n",
    "x = df.POINT_X\n",
    "y = df.POINT_Y\n",
    "\n",
    "df0.dropna(inplace = True)\n",
    "sizeref = 2.*max(df0['dave'])/(300)\n",
    "# f = px.scatter(df0, x='long', y='lat', color='sector2000_50_str', size='dave')\n",
    "# # senza colori per settore\n",
    "#f = px.scatter(df0, x='long', y='lat', size='dave', hover_data=['sector2000_50','dbase'])\n",
    "\n",
    "sizeref = 2.*max(df_zo.dbase)/300\n",
    "f = px.scatter(df_zo[(df_zo.Pds>=0.00) & (df_zo.epoch==3)], x='long', y='lat', size='dbase', color = 'dbase', hover_data=['sector2000_50','Pds'])\n",
    "\n",
    "\n",
    "f.update_traces(marker=dict(\n",
    "    sizemode='area', sizeref=sizeref, line_width=2))\n",
    "\n",
    "# f.add_trace(go.Scatter(x=df0.long, y=df0.lat, mode='markers', marker_size=df0.dave, marker=dict(\n",
    "#     sizemode='area', sizeref=sizeref, line_width=2)))\n",
    "\n",
    "# senza spessore punti\n",
    "# f.add_trace(go.Scatter(x=df_zo[(df_zo['dbase'] > 0) & (df_zo.Pds > 0.01)]['long'], y=df_zo[(\n",
    "#     df_zo['dbase'] > 0) & (df_zo.Pds > 0.01)]['lat'], mode='markers', marker=dict(size=0.5,color=2)))\n",
    "\n",
    "# con spessore punti\n",
    "# f.add_trace(go.Scatter(x=df_zo[df_zo['dbase'] > 0]['long'], y=df_zo[df_zo['dbase'] > 0]['lat'], mode='markers', marker_size=sizeref*10))\n",
    "\n",
    "f.add_trace(go.Scatter(x=x, y=y))\n",
    "\n",
    "f.add_trace(go.Scatter(x=vtrackup[:, 0], y=vtrackup[:, 1]))\n",
    "f.add_trace(go.Scatter(x=vtrackdown[:, 0], y=vtrackdown[:, 1]))\n",
    "\n",
    "f.add_trace(go.Scatter(x=vtrackup500[:, 0], y=vtrackup500[:, 1]))\n",
    "f.add_trace(go.Scatter(x=vtrackdown500[:, 0], y=vtrackdown500[:, 1]))\n",
    "\n",
    "f.add_trace(go.Scatter(x=vtrackup2000[:, 0], y=vtrackup2000[:, 1]))\n",
    "f.add_trace(go.Scatter(x=vtrackdown2000[:, 0], y=vtrackdown2000[:, 1]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f.update_yaxes(\n",
    "    scaleanchor = \"x\",\n",
    "    scaleratio = 1,\n",
    ")\n",
    "\n",
    "\n",
    "f.update_layout(\n",
    "    autosize=False,\n",
    "    width=1100,\n",
    "    height=800,\n",
    "    margin=dict(\n",
    "        l=0,\n",
    "        r=0,\n",
    "        b=0,\n",
    "        t=0,\n",
    "        pad=4\n",
    "    ), legend = dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.01\n",
    "    )\n",
    ")\n",
    "\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotto intere simulazioni solo subsidenze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "\n",
    "# tracciato principale\n",
    "x = df.POINT_X\n",
    "y = df.POINT_Y\n",
    "\n",
    "df0.dropna(inplace = True)\n",
    "sizeref = 2.*max(df0['dave'])/(300)\n",
    "# f = px.scatter(df0, x='long', y='lat', color='sector2000_50_str', size='dave')\n",
    "# # senza colori per settore\n",
    "#f = px.scatter(df0, x='long', y='lat', size='dave', hover_data=['sector2000_50','dbase'])\n",
    "\n",
    "sizeref = 2.*max(df_zo.dbase)/300\n",
    "f = px.scatter(df_zo[(df_zo.Pds>=0.01) & (df_zo.Pds< 0.1)], x='long', y='lat', size='dbase', color = 'dbase', hover_data=['sector2000_50','Pds'])\n",
    "\n",
    "\n",
    "f.update_traces(marker=dict(\n",
    "    sizemode='area', sizeref=sizeref, line_width=2))\n",
    "\n",
    "# f.add_trace(go.Scatter(x=df0.long, y=df0.lat, mode='markers', marker_size=df0.dave, marker=dict(\n",
    "#     sizemode='area', sizeref=sizeref, line_width=2)))\n",
    "\n",
    "# senza spessore punti\n",
    "# f.add_trace(go.Scatter(x=df_zo[(df_zo['dbase'] > 0) & (df_zo.Pds > 0.01)]['long'], y=df_zo[(\n",
    "#     df_zo['dbase'] > 0) & (df_zo.Pds > 0.01)]['lat'], mode='markers', marker=dict(size=0.5,color=2)))\n",
    "\n",
    "# con spessore punti\n",
    "# f.add_trace(go.Scatter(x=df_zo[df_zo['dbase'] > 0]['long'], y=df_zo[df_zo['dbase'] > 0]['lat'], mode='markers', marker_size=sizeref*10))\n",
    "\n",
    "f.add_trace(go.Scatter(x=x, y=y))\n",
    "\n",
    "f.add_trace(go.Scatter(x=vtrackup[:, 0], y=vtrackup[:, 1]))\n",
    "f.add_trace(go.Scatter(x=vtrackdown[:, 0], y=vtrackdown[:, 1]))\n",
    "\n",
    "f.add_trace(go.Scatter(x=vtrackup500[:, 0], y=vtrackup500[:, 1]))\n",
    "f.add_trace(go.Scatter(x=vtrackdown500[:, 0], y=vtrackdown500[:, 1]))\n",
    "\n",
    "f.add_trace(go.Scatter(x=vtrackup2000[:, 0], y=vtrackup2000[:, 1]))\n",
    "f.add_trace(go.Scatter(x=vtrackdown2000[:, 0], y=vtrackdown2000[:, 1]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f.update_yaxes(\n",
    "    scaleanchor = \"x\",\n",
    "    scaleratio = 1,\n",
    ")\n",
    "\n",
    "\n",
    "f.update_layout(\n",
    "    autosize=False,\n",
    "    width=1100,\n",
    "    height=800,\n",
    "    margin=dict(\n",
    "        l=0,\n",
    "        r=0,\n",
    "        b=0,\n",
    "        t=0,\n",
    "        pad=4\n",
    "    ), legend = dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.01\n",
    "    )\n",
    ")\n",
    "\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotto intere simulazioni doline + subsidenze - colore Pds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "\n",
    "# tracciato principale\n",
    "x = df.POINT_X\n",
    "y = df.POINT_Y\n",
    "\n",
    "df0.dropna(inplace = True)\n",
    "sizeref = 2.*max(df0['dave'])/(300)\n",
    "# f = px.scatter(df0, x='long', y='lat', color='sector2000_50_str', size='dave')\n",
    "# # senza colori per settore\n",
    "#f = px.scatter(df0, x='long', y='lat', size='dave', hover_data=['sector2000_50','dbase'])\n",
    "\n",
    "\n",
    "# #  tutte le simulazioni solo doline\n",
    "# sizeref = 2.*max(df_zo.dbase)/300\n",
    "# f = px.scatter(df_zo[(df_zo.Pds>=0.1)], x='long', y='lat', size='dbase', color = 'dbase', hover_data=['sector2000_50','Pds'])\n",
    "\n",
    "# #  tutte le simulazioni solo subsidenze\n",
    "# sizeref = 2.*max(df_zo.dbase)/300\n",
    "# f = px.scatter(df_zo[(df_zo.Pds>=0.01) & df_zo[(df_zo.Pds<0.1)], x='long', y='lat', size='dbase', color = 'dbase', hover_data=['sector2000_50','Pds'])\n",
    "\n",
    "#  una simulazione Pds >= 0.01\n",
    "sizeref = 2.*max(df_zo.dbase)/500\n",
    "f = px.scatter(df_zo[(df_zo.Pds>=0.01) & ((df_zo.epoch==10) | (df_zo.epoch==20))], x='long', y='lat', size='dbase', color = 'Pds', hover_data=['sector2000_50','Pds'])\n",
    "\n",
    "\n",
    "f.update_traces(marker=dict(\n",
    "    sizemode='area', sizeref=sizeref, line_width=2))\n",
    "\n",
    "# f.add_trace(go.Scatter(x=df0.long, y=df0.lat, mode='markers', marker_size=df0.dave, marker=dict(\n",
    "#     sizemode='area', sizeref=sizeref, line_width=2)))\n",
    "\n",
    "# senza spessore punti\n",
    "# f.add_trace(go.Scatter(x=df_zo[(df_zo['dbase'] > 0) & (df_zo.Pds > 0.01)]['long'], y=df_zo[(\n",
    "#     df_zo['dbase'] > 0) & (df_zo.Pds > 0.01)]['lat'], mode='markers', marker=dict(size=0.5,color=2)))\n",
    "\n",
    "# con spessore punti\n",
    "# f.add_trace(go.Scatter(x=df_zo[df_zo['dbase'] > 0]['long'], y=df_zo[df_zo['dbase'] > 0]['lat'], mode='markers', marker_size=sizeref*10))\n",
    "\n",
    "f.add_trace(go.Scatter(x=x, y=y))\n",
    "\n",
    "f.add_trace(go.Scatter(x=vtrackup[:, 0], y=vtrackup[:, 1]))\n",
    "f.add_trace(go.Scatter(x=vtrackdown[:, 0], y=vtrackdown[:, 1]))\n",
    "\n",
    "f.add_trace(go.Scatter(x=vtrackup500[:, 0], y=vtrackup500[:, 1]))\n",
    "f.add_trace(go.Scatter(x=vtrackdown500[:, 0], y=vtrackdown500[:, 1]))\n",
    "\n",
    "f.add_trace(go.Scatter(x=vtrackup2000[:, 0], y=vtrackup2000[:, 1]))\n",
    "f.add_trace(go.Scatter(x=vtrackdown2000[:, 0], y=vtrackdown2000[:, 1]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f.update_yaxes(\n",
    "    scaleanchor = \"x\",\n",
    "    scaleratio = 1,\n",
    ")\n",
    "\n",
    "\n",
    "f.update_layout(\n",
    "    autosize=False,\n",
    "    width=1100,\n",
    "    height=800,\n",
    "    margin=dict(\n",
    "        l=0,\n",
    "        r=0,\n",
    "        b=0,\n",
    "        t=0,\n",
    "        pad=4\n",
    "    ), legend = dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.01\n",
    "    )\n",
    ")\n",
    "\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "\n",
    "# tracciato principale\n",
    "x = df.POINT_X\n",
    "y = df.POINT_Y\n",
    "\n",
    "df0.dropna(inplace = True)\n",
    "sizeref = 2.*max(df0['dave'])/(300)\n",
    "# f = px.scatter(df0, x='long', y='lat', color='sector2000_50_str', size='dave')\n",
    "# # senza colori per settore\n",
    "#f = px.scatter(df0, x='long', y='lat', size='dave', hover_data=['sector2000_50','dbase'])\n",
    "\n",
    "sizeref = 2.*max(df_zo.dbase)/300\n",
    "f = px.scatter(df_zo_100[df_zo_100.epoch<10], x='long', y='lat', size='dbase', hover_data=['sector2000_50','Pds'])\n",
    "\n",
    "\n",
    "f.update_traces(marker=dict(\n",
    "    sizemode='area', sizeref=sizeref, line_width=2))\n",
    "\n",
    "# f.add_trace(go.Scatter(x=df0.long, y=df0.lat, mode='markers', marker_size=df0.dave, marker=dict(\n",
    "#     sizemode='area', sizeref=sizeref, line_width=2)))\n",
    "\n",
    "# senza spessore punti\n",
    "f.add_trace(go.Scatter(x=df_zo_100[(df_zo_100['dbase'] > 0) & (df_zo_100.Pds > 0)&(df_zo_100.epoch<10)]['long'], y=df_zo_100[(\n",
    "    df_zo_100['dbase'] > 0) & (df_zo_100.Pds > 0)&(df_zo_100.epoch<10)]['lat'], mode='markers', marker=dict(size=8,color=2)))\n",
    "\n",
    "# con spessore punti\n",
    "# f.add_trace(go.Scatter(x=df_zo[df_zo['dbase'] > 0]['long'], y=df_zo[df_zo['dbase'] > 0]['lat'], mode='markers', marker_size=sizeref*10))\n",
    "\n",
    "f.add_trace(go.Scatter(x=x, y=y))\n",
    "\n",
    "f.add_trace(go.Scatter(x=vtrackup[:, 0], y=vtrackup[:, 1]))\n",
    "f.add_trace(go.Scatter(x=vtrackdown[:, 0], y=vtrackdown[:, 1]))\n",
    "\n",
    "f.add_trace(go.Scatter(x=vtrackup500[:, 0], y=vtrackup500[:, 1]))\n",
    "f.add_trace(go.Scatter(x=vtrackdown500[:, 0], y=vtrackdown500[:, 1]))\n",
    "\n",
    "f.add_trace(go.Scatter(x=vtrackup2000[:, 0], y=vtrackup2000[:, 1]))\n",
    "f.add_trace(go.Scatter(x=vtrackdown2000[:, 0], y=vtrackdown2000[:, 1]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f.update_yaxes(\n",
    "    scaleanchor = \"x\",\n",
    "    scaleratio = 1,\n",
    ")\n",
    "\n",
    "\n",
    "f.update_layout(\n",
    "    autosize=False,\n",
    "    width=1100,\n",
    "    height=600,\n",
    "    margin=dict(\n",
    "        l=0,\n",
    "        r=0,\n",
    "        b=0,\n",
    "        t=0,\n",
    "        pad=4\n",
    "    ), legend = dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.01\n",
    "    )\n",
    ")\n",
    "\n",
    "f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g50.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_g50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREO KARST STOCHASTICI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creo colonne dei boundary 100m 200m e 2000m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  CONVERSIONE IN COORDINATE UTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj import Proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tuple(df.POINT_X)\n",
    "y = tuple(df.POINT_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myProj = Proj(\"+proj=utm +zone=38N, +north +ellps=WGS84 +datum=WGS84 +units=m +no_defs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon, lat = myProj(x, y, inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = px.scatter_mapbox(lat=lat, lon=lon, zoom=8, height=300)\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_mapbox(lat=lat, lon=lon, zoom=7, height=300)\n",
    "fig.update_layout(\n",
    "    mapbox_style=\"white-bg\",\n",
    "    mapbox_layers=[\n",
    "        {\n",
    "            \"below\": 'traces',\n",
    "            \"sourcetype\": \"raster\",\n",
    "            \"sourceattribution\": \"United States Geological Survey\",\n",
    "            \"source\": [\n",
    "                \"https://basemap.nationalmap.gov/arcgis/rest/services/USGSImageryOnly/MapServer/tile/{z}/{y}/{x}\"\n",
    "            ]\n",
    "        }\n",
    "      ])\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
